# -*- coding: utf-8 -*-
"""Total_BP_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JC-EsPUE4tJmKnLyn74ZK3IxK4oP-0ds
"""

import numpy as np
import scipy
import networkx as nx
import pickle
import pandas as pd
from tqdm import tqdm
import os
import sklearn
from scipy.stats import linregress
from sklearn.preprocessing import StandardScaler
from scipy.optimize import curve_fit
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from matplotlib.lines import Line2D
import matplotlib.colors as mcolors
from scipy.stats import gaussian_kde

from google.colab import drive
drive.mount('/content/drive/')

df1 = pd.read_pickle("/content/drive/MyDrive/erdos_renyi_a-g_with_graph_features.pkl")
df2 = pd.read_pickle("/content/drive/MyDrive/N21-25_p1-3_with_graph_features.pkl")
df3 = pd.read_pickle("/content/drive/MyDrive/all_dense_a_and_b_with_graph_features.pkl")
df = pd.concat([df1, df2, df3], ignore_index=True)

base_fname = "bigset"

def analyze_feature_regressions(
    df,
    feature,
    regvar,
    min_cluster_size=8,
    min_samples=None,
    min_points=4,
    min_unique_N = 4
):
    """
    Perform linear regression of log(mean_grad_variance) vs N
    for each value of a discrete feature or each cluster in a continuous feature,
    segregated by optimizer and QAOA depth p. Saves the adjacency matrices used.

    Ensures that continuous-feature clusters span more than one distinct N value
    by pre-marking invalid clusters as noise before regression.
    """
    results = []

    is_integer = pd.api.types.is_integer_dtype(df[feature]) or all(
        df[feature].dropna() == df[feature].dropna().astype(int)
    )

    for optimizer in df['optimizer'].unique():
        df_opt = df[df['optimizer'] == optimizer]

        for p_val in sorted(df_opt['p'].unique()):
            df_p = df_opt[df_opt['p'] == p_val]

            if is_integer:
                grouped = df_p.groupby(feature)
                for feat_val, group in grouped:
                    if (
                        len(group) < min_points
                        or any(group[regvar] <= 0)
                        or group['N'].nunique() < min_unique_N  # avoid single-x regression
                    ):
                        continue

                    log_y = np.log(group[regvar].values)
                    slope, intercept, r, _, _ = linregress(group['N'].values, log_y)

                    results.append({
                        "feature": feature,
                        "feature_type": "discrete",
                        "cluster_label": f"val_{feat_val}",
                        "optimizer": optimizer,
                        "p": p_val,
                        "pearson_r_log": r,
                        "exp_a": np.exp(intercept),
                        "exp_b": slope,
                        "min_val": feat_val,
                        "max_val": feat_val,
                        "mean_val": feat_val,
                        "std_dev": 0,
                        "n_points": len(group),
                        "adjs": group["adj"].tolist()
                    })

            else:
                if len(df_p) < min_cluster_size:
                    continue

                X_scaled = StandardScaler().fit_transform(df_p[[feature]])
                hdb = sklearn.cluster.HDBSCAN(
                    min_cluster_size=min_cluster_size,
                    min_samples=min_samples or min_cluster_size // 2
                )
                labels = hdb.fit_predict(X_scaled)

                df_p = df_p.copy()
                df_p['cluster'] = labels

                # Pre-mark invalid clusters as noise
                for cid in set(labels):
                    if cid == -1:
                        continue
                    group = df_p[df_p['cluster'] == cid]
                    if (
                        len(group) < min_points
                        or any(group[regvar] <= 0)
                        or group['N'].nunique() < min_unique_N
                    ):
                        df_p.loc[df_p['cluster'] == cid, 'cluster'] = -1

                # Now loop over only valid clusters
                for cluster_id in sorted(set(df_p['cluster'])):
                    if cluster_id == -1:
                        continue

                    group = df_p[df_p['cluster'] == cluster_id]
                    log_y = np.log(group[regvar].values)
                    slope, intercept, r, _, _ = linregress(group['N'].values, log_y)

                    results.append({
                        "feature": feature,
                        "feature_type": "continuous",
                        "cluster_label": f"cluster_{cluster_id}",
                        "optimizer": optimizer,
                        "p": p_val,
                        "pearson_r_log": r,
                        "exp_a": np.exp(intercept),
                        "exp_b": slope,
                        "min_val": group[feature].min(),
                        "max_val": group[feature].max(),
                        "mean_val": group[feature].mean(),
                        "std_dev": group[feature].std(),
                        "n_points": len(group),
                        "adjs": group["adj"].tolist()
                    })

    return pd.DataFrame.from_records(results)

features = ['Number of edges', 'Edge density',
       'Mean degree', 'Standard deviation of degrees', 'Skewness of degrees',
       'Minimum degree', 'Maximum degree', 'Diameter', 'Radius',
       'Vertex connectivity', 'Edge connectivity',
       'Global clustering coefficient', 'Mean local clustering coefficient',
       'Standard deviation of local clustering coefficients',
       'Skewness of local clustering coefficients',
       'Minimum local clustering coefficient',
       'Maximum local clustering coefficient', 'Treewidth',
       'Average path length', 'Circuit rank', 'Girth',
       'Mean betweenness centrality',
       'Standard deviation of betweenness centralities',
       'Skewness of betweenness centralities',
       'Minimum betweenness centrality', 'Maximum betweenness centrality',
       'Algebraic connectivity', 'Von Neumann entropy',
       'Adjacency spectrum mean', 'Adjacency spectrum standard deviation',
       'Adjacency spectrum skewness', 'Adjacency spectrum min',
       'Adjacency spectrum max', 'Laplacian spectrum mean',
       'Laplacian spectrum standard deviation', 'Laplacian spectrum skewness',
       'Laplacian spectrum min', 'Laplacian spectrum max', 'Planarity',
       'Mean harmonic centrality',
       'Standard deviation of harmonic centralities',
       'Skewness of harmonic centralities', 'Minimum harmonic centrality',
       'Maximum harmonic centrality', 'Harmonic diameter', 'Mean core number',
       'Standard deviation of core numbers', 'Skewness of core numbers',
       'Minimum core number', 'Maximum core number', 'Chordality',
       'Haemers bound', 'Claw-free']

def run_complete_analysis(qaoa_df, base_name, gfeatures=features, regvars=['mean_grad_variance']):

    merged = qaoa_df.copy()
    feature_cols = [feat for feat in gfeatures if feat not in ['adj', 'adj_key', 'Number of vertices', 'N']]
    results_dict = {var: [] for var in regvars}

    total_jobs = sum(
        len(merged[merged["optimizer"] == optimizer]["p"].unique())
        for optimizer in merged['optimizer'].unique()
    )
    outer_loop = tqdm(total=total_jobs, desc="Analyzing optimizers/p", position=0)

    for opt_name in merged['optimizer'].unique():
        df_opt = merged[merged['optimizer'] == opt_name]

        for p_val in df_opt["p"].unique():
            df_p = df_opt[df_opt["p"] == p_val]

            # Single progress bar for all regvars * features for this (opt_name, p_val)
            total_inner = len(regvars) * len(feature_cols)
            inner_loop = tqdm(total=total_inner, desc=f"{opt_name}, p={p_val}", leave=False, position=1)

            for regvar in regvars:
                for feat in feature_cols:
                    results_df = analyze_feature_regressions(
                        df_p,
                        feat,
                        regvar,
                        min_cluster_size=max(5, len(df_p) // 60),
                        min_points=max(2, len(df_p) // 120)
                    )

                    if results_df is not None and not results_df.empty:
                        results_df = results_df.copy()
                        results_df['optimizer'] = opt_name
                        results_df['p'] = p_val
                        results_dict[regvar].append(results_df)

                    inner_loop.update(1)

            inner_loop.close()
            outer_loop.update(1)

    outer_loop.close()

    for regvar in regvars:
        if results_dict[regvar]:
            corr_df = pd.concat(results_dict[regvar], ignore_index=True)
            corr_df = corr_df.sort_values(by='pearson_r_log', ascending=True).reset_index(drop=True)

            corr_df.to_pickle(base_name + f'feature_correlations_{regvar}.pkl')
        else:
            print("No correlation results were produced for " + regvar + ".")

    return results_dict

corr_feat_df = run_complete_analysis(df, base_fname,
                                     gfeatures=features, regvars = ['mean_grad_variance'])
corr_feat_df = corr_feat_df['mean_grad_variance']
corr_feat_df = pd.concat(corr_feat_df, ignore_index=True)
corr_feat_df = corr_feat_df.sort_values(by='pearson_r_log', ascending=True).reset_index(drop=True)

# Now you can slice columns and print:
print(corr_feat_df[['feature', 'pearson_r_log']].head(20))

print(corr_feat_df[['feature', 'pearson_r_log', 'n_points']].head(100))

print(corr_feat_df[['feature', 'feature_type', 'p', 'pearson_r_log', 'n_points', 'min_val',
                    'max_val', 'mean_val', 'std_dev']].head(40))

import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import matplotlib.cm as cm
import seaborn as sns
import numpy as np
import pandas as pd
import matplotlib.font_manager as fm
from matplotlib.lines import Line2D


def plot_N_vs_mean_grad_variance_with_fit(qaoa_df, corr_feat_df, top_n=10, ascend=True):
    font_prop = fm.FontProperties(fname='/content/drive/MyDrive/cmunbmr.ttf')  # replace with actual path

    qaoa_df = qaoa_df.copy()
    qaoa_df['adj_str'] = qaoa_df['adj'].apply(lambda x: str(x))

    if ascend:
        top_clusters = corr_feat_df.head(top_n)
    else:
        top_clusters = corr_feat_df.tail(top_n)

    for idx, cluster_row in top_clusters.iterrows():
        p_val = cluster_row['p']
        cluster_adjs = cluster_row['adjs']
        feature_name = cluster_row["feature"]
        feature_type = cluster_row.get("feature_type", "continuous")
        cluster_adj_strs = set(str(adj) for adj in cluster_adjs)

        candidate_rows = qaoa_df[qaoa_df['p'] == p_val]
        matched = candidate_rows[candidate_rows['adj_str'].isin(cluster_adj_strs)].copy()

        if matched.empty:
            print(f"Cluster {idx} has no matching points, skipping plot.")
            continue

        grouped_stats = matched.groupby('N').agg(
            mean_grad_variance_mean=('mean_grad_variance', 'mean'),
            mean_grad_variance_std=('mean_grad_variance', 'std'),
            feature_mean=(feature_name, 'mean'),
            feature_std=(feature_name, 'std'),
            count=('N', 'count')
        ).reset_index()

        total_points = grouped_stats['count'].sum()
        grouped_stats['proportion'] = grouped_stats['count'] / total_points * 100

        feature_is_int = pd.api.types.is_integer_dtype(matched[feature_name])

        a = cluster_row.get('exp_a', None)
        b = cluster_row.get('exp_b', None)
        r = cluster_row.get('pearson_r_log', None)

        # Print Pearson r and exponential fit instead of displaying on plot
        print(f"Cluster {idx}: Pearson r = {r:.3f}")
        if a is not None and b is not None:
            print(f"Cluster {idx}: Exponential fit: {a:.3g} * exp({b:.3g} * N)")

        multi_mask = grouped_stats['count'] > 1
        singleton_mask = grouped_stats['count'] == 1

        norm = mcolors.Normalize(
            vmin=grouped_stats.loc[multi_mask, 'proportion'].min() if multi_mask.any() else 0,
            vmax=grouped_stats.loc[multi_mask, 'proportion'].max() if multi_mask.any() else 100
        )
        cmap = cm.viridis

        with plt.style.context('seaborn-v0_8-paper'):
            plt.figure(figsize=(10, 6))

            # Plot scatter points
            for _, row in grouped_stats.iterrows():
                if row['count'] == 1:
                    plt.scatter(row['N'], row['mean_grad_variance_mean'],
                                c='white', edgecolor='k', alpha=0.6, s=120)
                else:
                    plt.scatter(row['N'], row['mean_grad_variance_mean'],
                                c=[cmap(norm(row['proportion']))], edgecolor='k', s=120)

            # Error bars without blue center points
            if multi_mask.any():
                plt.errorbar(
                    grouped_stats.loc[multi_mask, 'N'],
                    grouped_stats.loc[multi_mask, 'mean_grad_variance_mean'],
                    yerr=grouped_stats.loc[multi_mask, 'mean_grad_variance_std'],
                    fmt='none', capsize=12, ecolor='black', elinewidth=1
                )

            # Exponential fit curve
            if a is not None and b is not None:
                N_fit = np.linspace(grouped_stats['N'].min(), grouped_stats['N'].max(), 200)
                y_fit = a * np.exp(b * N_fit)
                plt.plot(N_fit, y_fit, 'r--', linewidth=3, label=f'Fit: {a:.3g} * exp({b:.3g} * N)')

            plt.xlabel('N (Number of vertices)', fontsize=27, fontproperties=font_prop, labelpad=20)
            plt.ylabel('Mean Gradient Variance', fontsize=27, fontproperties=font_prop, labelpad=20)
            plt.grid(True, alpha=.25)
            plt.tick_params(axis='both', labelsize=12)

            # Title commented out as per previous request
            # plt.title(
            #     f"N vs Mean Gradient Variance\n"
            #     f"Cluster {idx}, p={p_val}, Feature={feature_name}\n"
            #     f"{title_feature_info}\n"
            #     f"Pearson r = {r:.3f}"
            # )

            if feature_type == "continuous":
                title_feature_info = (
                    f"Feature mean: {cluster_row.get('mean_val', np.nan):.3g}, "
                    f"Min: {cluster_row.get('min_val', np.nan):.3g}, "
                    f"Max: {cluster_row.get('max_val', np.nan):.3g}, "
                    f"Cluster Size: {cluster_row.get('n_points', np.nan):.3g}"
                )
            else:
                title_feature_info = (f"Feature value: {cluster_row.get('feature_value', 'N/A')}, "
                                      f"Cluster Size: {cluster_row.get('n_points', np.nan):.3g}")

            # Colorbar with bigger font
            if multi_mask.any():
                sm = cm.ScalarMappable(norm=norm, cmap=cmap)
                sm.set_array([])
                cbar = plt.colorbar(sm, ax=plt.gca(), label='% of cluster points at N')
                cbar.ax.yaxis.label.set_fontproperties(font_prop)  # Apply supplied font
                cbar.ax.yaxis.label.set_fontsize(18)  # Bigger font size for colorbar

            legend_elements = []
            if singleton_mask.any():
                legend_elements.append(Line2D([0], [0], marker='o', color='w',
                                              label='Singleton N value (faded)',
                                              markerfacecolor='white', markeredgecolor='k',
                                              markersize=8, alpha=0.6))

            if a is not None and b is not None:
                legend_elements.append(Line2D([0], [0], color='r', linestyle='--',
                                              label=f'Fit: {a:.3g} * exp({b:.3g} * N)'))

            if legend_elements:
                plt.legend(handles=legend_elements, loc='upper left')

            plt.tight_layout()
            plt.show()

plot_N_vs_mean_grad_variance_with_fit(df, corr_feat_df, top_n=10)

print(corr_feat_df[['feature', 'pearson_r_log', 'n_points']].tail(100))

print(corr_feat_df[['feature', 'feature_type', 'p', 'pearson_r_log', 'n_points', 'min_val',
                    'max_val', 'mean_val', 'std_dev']].tail(40))

plot_N_vs_mean_grad_variance_with_fit(df, corr_feat_df, top_n=20, ascend=False)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import cm
from scipy.ndimage import gaussian_filter1d
from collections import defaultdict
import hashlib

def hash_adj(adj):
    if isinstance(adj, np.ndarray):
        return hashlib.md5(adj.tobytes()).hexdigest()
    else:
        return hashlib.md5(str(adj).encode('utf-8')).hexdigest()

def plot_ridge_for_feature_clusters(qaoa_df, corr_feat_df, feature_name, bins=80, smooth_sigma=1):
    # Copy data to avoid modifying originals
    qaoa_df = qaoa_df.copy()
    corr_feat_df = corr_feat_df.copy()

    # Hash adjacency matrices in qaoa_df for fast lookup
    qaoa_df['adj_hash'] = qaoa_df['adj'].apply(hash_adj)

    # Build lookup: (adj_hash, p) -> list of mean_grad_variance values
    adj_p_lookup = defaultdict(list)
    for _, row in qaoa_df.iterrows():
        key = (row['adj_hash'], row['p'])
        adj_p_lookup[key].append(row['mean_grad_variance'])

    # Prepare data for each cluster
    cluster_data = []
    for idx, cluster_row in corr_feat_df.iterrows():
        p_val = cluster_row['p']
        pearson_r = cluster_row['pearson_r_log']
        cluster_label = cluster_row.get('cluster_label', f'Cluster {idx}')
        adjs = cluster_row['adjs']
        cluster_adj_hashes = set(hash_adj(adj) for adj in adjs)

        # Gather all matching mean_grad_variance values for this cluster and p
        vals = []
        for adj_hash in cluster_adj_hashes:
            vals.extend(adj_p_lookup.get((adj_hash, p_val), []))

        if not vals:
            print(f"Skipping cluster '{cluster_label}' - no data found.")
            continue

        cluster_data.append({
            'label': cluster_label,
            'values': np.array(vals),
            'pearson_r_log': pearson_r
        })

    if not cluster_data:
        print("No clusters with data to plot.")
        return

    df_clusters = pd.DataFrame(cluster_data)

    # Sort clusters by pearson_r_log descending to have consistent order
    df_clusters = df_clusters.sort_values('pearson_r_log', ascending=False).reset_index(drop=True)

    # Color mapping
    norm = plt.Normalize(df_clusters['pearson_r_log'].min(), df_clusters['pearson_r_log'].max())
    cmap = cm.viridis

    # Determine common x-axis range with padding
    all_vals = np.hstack(df_clusters['values'].values)
    x_min, x_max = np.min(all_vals), np.max(all_vals)
    x_range = x_max - x_min
    pad = x_range * 0.1 if x_range > 0 else 1  # 10% padding or 1 if range=0
    x_start, x_end = x_min - pad, x_max + pad
    bins_edges = np.linspace(x_start, x_end, bins + 1)
    bins_centers = (bins_edges[:-1] + bins_edges[1:]) / 2

    fig, ax = plt.subplots(figsize=(10, 1 + len(df_clusters) * 0.8))

    # Plot each cluster as a ridge
    for i, row in df_clusters.iterrows():
        vals = row['values']

        counts, _ = np.histogram(vals, bins=bins_edges, density=True)
        smooth_counts = gaussian_filter1d(counts, sigma=smooth_sigma)

        # Normalize height to max=1 so ridges are comparable
        if smooth_counts.max() > 0:
            smooth_counts /= smooth_counts.max()

        y = smooth_counts + i  # vertical offset by cluster index

        color = cmap(norm(row['pearson_r_log']))
        ax.fill_between(bins_centers, i, y, color=color, alpha=0.75)
        ax.plot(bins_centers, y, color=color, lw=1.5)

    ax.set_yticks(np.arange(len(df_clusters)))
    ax.set_yticklabels(df_clusters['label'])
    ax.set_xlabel(feature_name)
    ax.set_ylabel('Clusters')
    ax.set_title(f'Ridge plot of clusters for feature "{feature_name}" colored by Pearson r')

    # Colorbar for Pearson r
    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
    sm.set_array([])
    plt.colorbar(sm, ax=ax, label='Pearson r')

    plt.tight_layout()
    plt.show()

import numpy as np
import pandas as pd
import hashlib
from collections import defaultdict

def hash_adj(adj):
    """Generates a hash for a numpy array adjacency matrix."""
    if isinstance(adj, np.ndarray):
        return hashlib.md5(adj.tobytes()).hexdigest()
    else:
        # Fallback for non-numpy arrays, though expects numpy
        return hashlib.md5(str(adj).encode('utf-8')).heigest()


def extract_cluster_values(df, corr_feat_df, value_column='mean_grad_variance'):
    """
    Extracts values from a specified column (e.g., 'mean_grad_variance')
    and the feature values for all clusters from corr_feat_df
    by matching adjacency matrices and p-values with df.

    Args:
        df (pd.DataFrame): The main DataFrame containing 'adj', 'p', 'mean_grad_variance', and feature columns.
        corr_feat_df (pd.DataFrame): DataFrame with cluster information, including 'adjs', 'p', and 'feature'.
        value_column (str): The name of the column in df whose values should be extracted for each cluster.
                            Defaults to 'mean_grad_variance'.

    Returns:
        pd.DataFrame: A copy of corr_feat_df with added columns:
                      - f'{value_column}_values' containing lists of values for each cluster.
                      - f'{feature}_values' for the specific feature of that cluster.
                      Returns an empty DataFrame if no clusters are in corr_feat_df or
                      the value_column is not found in df.
    """
    if corr_feat_df.empty:
        print("Input corr_feat_df is empty.")
        return pd.DataFrame()

    if value_column not in df.columns:
        print(f"Error: Column '{value_column}' not found in the main DataFrame.")
        return pd.DataFrame()

    # Hash adjacency matrices in df for fast lookup
    df_hashed = df.copy()
    df_hashed['adj_hash'] = df_hashed['adj'].apply(hash_adj)

    # Build lookup: (adj_hash, p) -> index in df_hashed for quick access
    # This avoids iterating through all rows of df_hashed repeatedly
    adj_p_index_lookup = defaultdict(list)
    for idx, row in df_hashed.iterrows():
        key = (row['adj_hash'], row['p'])
        adj_p_index_lookup[key].append(idx)


    # Add value columns to corr_feat_df
    corr_df_with_values = corr_feat_df.copy()
    value_list = []
    feature_value_list = []

    for idx, cluster_row in corr_df_with_values.iterrows():
        p_val_cluster = cluster_row['p']
        feature_name_cluster = cluster_row['feature'] # Get the specific feature name for this cluster
        cluster_adjs = cluster_row['adjs']
        cluster_adj_hashes = set(hash_adj(adj) for adj in cluster_adjs)

        # Check if the feature column exists in the main DataFrame
        if feature_name_cluster not in df.columns:
             print(f"Warning: Feature column '{feature_name_cluster}' not found in the main DataFrame for cluster {idx}. Skipping feature value extraction for this cluster.")
             value_list.append([]) # Append empty list for value_column if feature is missing
             feature_value_list.append([]) # Append empty list for feature values
             continue


        # Gather all matching values for this cluster and p
        vals = []
        feat_vals = []

        for adj_hash in cluster_adj_hashes:
            # Get the indices in df_hashed that match this adj_hash and p_val
            matching_indices = adj_p_index_lookup.get((adj_hash, p_val_cluster), [])

            for df_idx in matching_indices:
                 # Use the index to get the row from the original df (or df_hashed)
                 # Ensure the row actually has the feature and value columns
                 row_data = df.loc[df_idx] # Use original df to avoid potential issues with added columns

                 if value_column in row_data:
                    vals.append(row_data[value_column])
                 if feature_name_cluster in row_data:
                    feat_vals.append(row_data[feature_name_cluster])

        value_list.append(vals)
        feature_value_list.append(feat_vals)


    corr_df_with_values[f'{value_column}_values'] = value_list
    # Name the feature value column dynamically based on the feature name for the cluster
    # Note: This will overwrite the column if feature_name_cluster is the same for different rows,
    # but since we group by feature and p in the plotting function, this is acceptable.
    # A better approach might be to store feature values in a dictionary or list column.
    # Let's store them in a list column named after the feature for simplicity in this context.
    # We'll create a column named f'{feature_name_cluster}_values' for each feature.
    # This means the DataFrame will have multiple columns for feature values if corr_feat_df has multiple features.
    # The plotting function will need to handle this by selecting the correct column based on the loop's feature_name.
    # Let's add a generic column and let the plotting function pick the right values.
    # Sticking to the original plan of adding a column named dynamically:

    # Let's refine this: We need a way to access the feature values *for the specific feature of that row*.
    # The current approach of adding a column named f'{feature_name_cluster}_values' won't work
    # because 'feature_name_cluster' changes row by row.
    # Instead, let's add *one* column, say 'feature_values_list', and populate it with the list of values
    # for the feature specified in the 'feature' column of that specific row.

    corr_df_with_values['feature_values_list'] = feature_value_list


    return corr_df_with_values



# Use the updated function to extract both mean_grad_variance and feature values
corr_feat_df_grads = extract_cluster_values(df, corr_feat_df, value_column='mean_grad_variance')
display(corr_feat_df_grads.head())

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import cm
from scipy.ndimage import gaussian_filter1d
import seaborn as sns
from scipy.stats import gaussian_kde # Import gaussian_kde

def plot_ridge_plots_by_p_and_feature(corr_feat_df_grads, bins=80, smooth_sigma=1, spacing_factor=1.2, min_fig_height=5):
    """
    Generates ridge plots for each distinct p and feature combination in
    corr_feat_df_grads. The x-axis represents feature values,
    and each ridge corresponds to a cluster, colored by its Pearson correlation coefficient.
    Clusters are ordered by increasing mean feature value.
    Labels indicate the rank in the overall Pearson r sorted dataframe,
    including the exponent coefficient and number of points.

    Handles singular matrix error in gaussian_kde by plotting a simple histogram
    or scatter point for clusters with identical or insufficient data points,
    and annotates them with the Pearson r value and exponent coefficient (for constant values). Uses consistent coloring based on Pearson r range [-1, 1].

    Args:
        corr_feat_df_grads (pd.DataFrame): DataFrame containing cluster information,
                                          including 'p', 'feature', 'pearson_r_log', 'exp_b',
                                          'mean_grad_variance_values', 'feature_values_list',
                                          'mean_val', and 'n_points'. Assumes this DataFrame is
                                          already sorted by 'pearson_r_log'.
        bins (int): Number of bins for the histogram.
        smooth_sigma (float): Standard deviation for Gaussian smoothing of the distributions.
        spacing_factor (float): Factor greater than 1 determines the vertical spacing between ridges.
        min_fig_height (float): Minimum height for each generated figure.
    """
    if corr_feat_df_grads.empty:
        print("Input corr_feat_df_grads is empty. Cannot generate plots.")
        return

    # Consistent color mapping based on Pearson r range [-1, 1]
    norm = plt.Normalize(-1, 1)
    cmap = cm.viridis


    for (p_val, feature_name), group_df in corr_feat_df_grads.groupby(['p', 'feature']):
        # Removed the print statement here

        feature_values_col = 'feature_values_list' # This column holds the feature values

        # Filter out clusters with no feature data
        # Keep the original index (Pearson r rank)
        group = group_df[group_df[feature_values_col].apply(lambda x: len(x) > 0)].copy()

        if group.empty:
            print(f"  No clusters with {feature_name} data for p={p_val}, Feature='{feature_name}'. Skipping.")
            continue

        # Store the original index (Pearson r rank) before sorting by mean_val
        group['original_rank'] = group.index

        # Sort clusters by mean feature value ascending for plotting order
        group = group.sort_values('mean_val', ascending=True).reset_index(drop=True)

        # Determine common x-axis range for this p and feature combination based on feature values
        all_vals_in_group = np.hstack(group[feature_values_col].values)
        if all_vals_in_group.size == 0:
             print(f"  No {feature_name} values found for p={p_val}, Feature='{feature_name}'. Skipping.")
             continue

        x_min, x_max = np.min(all_vals_in_group), np.max(all_vals_in_group)
        x_range = x_max - x_min
        pad = x_range * 0.1 if x_range > 0 else 1 # 10% padding or 1 if range=0
        x_start, x_end = x_min - pad, x_max + pad
        bins_edges = np.linspace(x_start, x_end, bins + 1)
        bins_centers = (bins_edges[:-1] + bins_edges[1:]) / 2

        # Adjust figure size based on number of clusters and spacing factor, with a minimum height
        calculated_height = 1 + (len(group) - 1) * spacing_factor if len(group) > 0 else 1
        fig_height = max(min_fig_height, calculated_height)
        fig, ax = plt.subplots(figsize=(10, fig_height * 0.6)) # Adjusted base height scaling slightly

        # Plot each cluster as a ridge - iterate in sorted order (by mean_val) for drawing priority
        y_tick_labels = []
        y_tick_positions = []
        for i, row in group.iterrows():
            vals = np.array(row[feature_values_col])
            n_points = row['n_points'] # Get n_points for label
            cluster_label = row.get('cluster_label', f'Cluster {i}')
            pearson_r = row['pearson_r_log'] # Get Pearson r for annotation
            exp_b = row['exp_b'] # Get exp_b for label and annotation
            color = cmap(norm(pearson_r)) # Use the consistent norm here

            # Use the stored original rank for the label
            original_rank = row['original_rank']
            # Include original index/rank, exp_b, and n_points in the label
            y_tick_labels.append(f"{original_rank} (b={exp_b:.2f}, n={n_points})")
            y_tick_positions.append(i * spacing_factor + 0.5) # Position labels in the middle of the ridge

            if n_points == 0:
                continue # Should have been filtered out, but just in case

            # Calculate vertical offset for the base of the ridge
            vertical_offset = i * spacing_factor

            try:
                # Try to use KDE if there's enough unique data points
                if np.unique(vals).size > 1 and n_points > 1:
                    kde = gaussian_kde(vals)
                    density = kde(bins_centers)
                    # Normalize height to max=1 so ridges are comparable
                    if density.max() > 0:
                        density /= density.max()
                    y = density + vertical_offset # vertical offset
                    ax.fill_between(bins_centers, vertical_offset, y, color=color, alpha=0.75)
                    ax.plot(bins_centers, y, color=color, lw=1.5)
                elif n_points > 0:
                    # If all values are the same or only one point, use histogram/scatter
                    if np.unique(vals).size == 1 and n_points > 1:
                        # Multiple points with the same value - use a simple vertical line
                        single_val = vals[0]
                        bin_idx = np.digitize([single_val], bins_edges)[0] - 1
                        if 0 <= bin_idx < len(bins_centers):
                             # Plot a vertical line scaled to ridge height
                             ax.plot([bins_centers[bin_idx], bins_centers[bin_idx]], [vertical_offset, vertical_offset + 1.0], color=color, lw=3) # Increased lw
                             # Add annotation for Pearson r and exp_b
                             ax.text(bins_centers[bin_idx], vertical_offset + 0.5,
                                     f'r={pearson_r:.2f}', # Removed b={exp_b:.2f} and n={n_points}
                                     va='center', ha='left', fontsize=8, color='black')

                    elif n_points == 1:
                        # Single point - use a scatter marker
                        ax.plot(vals[0], vertical_offset + 0.5, 'o', color=color, markeredgecolor='k', markersize=8, alpha=0.9) # Increased markersize/alpha
                        # Add annotation for Pearson r
                        ax.text(vals[0], vertical_offset + 0.5,
                                f'r={pearson_r:.2f}', # Removed n=1
                                va='center', ha='left', fontsize=8, color='black')
                    else: # n_points > 0 but unique values > 1 (should be covered by KDE, but as fallback)
                        # This case should ideally not be reached if KDE is attempted first
                        # but as a fallback, plot points and simple annotation
                        ax.plot(vals, np.full_like(vals, vertical_offset + 0.5), 'o', color=color, markeredgecolor='k', markersize=5, alpha=0.8)
                        ax.text(vals[0] if n_points > 0 else x_start, vertical_offset + 0.5,
                                f'r={pearson_r:.2f}', # Removed n={n_points}
                                va='center', ha='left', fontsize=8, color='black')


            except Exception as e:
                print(f"  Error processing cluster '{cluster_label}' (p={p_val}, feature='{feature_name}'): {e}. Skipping or using fallback.")
                 # Fallback to plotting raw points if any other error occurs (e.g., KDE fails for other reasons)
                if n_points > 0:
                     ax.plot(vals, np.full_like(vals, vertical_offset + 0.5), 'o', color=color, markeredgecolor='k', markersize=8, alpha=0.9)
                     # Add annotation for Pearson r and exp_b on fallback if constant value
                     annotation_text = f'r={pearson_r:.2f}' # Default annotation
                     if np.unique(vals).size == 1:
                          annotation_text = f'r={pearson_r:.2f}' # Removed b={exp_b:.2f}
                     ax.text(vals[0] if n_points > 0 else x_start, vertical_offset + 0.5,
                            annotation_text,
                            va='center', ha='left', fontsize=8, color='black')


        ax.set_yticks(y_tick_positions) # Use calculated positions for labels
        ax.set_yticklabels(y_tick_labels) # Use the generated labels
        ax.set_xlabel(feature_name) # X-axis label is the feature name
        ax.set_ylabel('Clusters (Rank in Pearson r)')
        ax.set_title(f'Ridge Plot of {feature_name} by Cluster\n(p={p_val}), colored by Pearson r (Mean Grad Variance vs N)')

        # Colorbar for Pearson r
        sm = plt.cm.ScalarMappable(norm=norm, cmap=cmap) # Use the consistent norm here
        sm.set_array([])
        plt.colorbar(sm, ax=ax, label='Pearson r (Mean Grad Variance vs N)')

        # Adjust y-limits based on number of clusters and spacing factor
        ax.set_ylim(-0.5, (len(group) - 1) * spacing_factor + 1.5) # Add some padding at the top

        plt.tight_layout()
        plt.show()

corr_feat_df_vals = corr_feat_df_grads.copy()
del corr_feat_df_grads # Optional: remove the old variable to free memory
display(corr_feat_df_vals.head())

plot_ridge_plots_by_p_and_feature(corr_feat_df_vals)



df_filtered = df[df['mean_grad_variance'] > 0].copy()
x_data = df_filtered['N']
y_data = df_filtered['mean_grad_variance']

def exponential_func(x, a, b):
    return a * np.exp(b * x)

popt, pcov = curve_fit(exponential_func, x_data, y_data)

A, B = popt

plt.figure(figsize=(10, 6))
plt.scatter(x_data, y_data, label='Original Data', alpha=0.6)

x_fit = np.linspace(x_data.min(), x_data.max(), 200)
y_fit = exponential_func(x_fit, A, B)
plt.plot(x_fit, y_fit, color='red', label=f'Fit: y = {A:.2e} * exp({B:.2f} * N)')

plt.yscale('log')
plt.title('Exponential Fit of Mean Gradient Variance vs N')
plt.xlabel('N')
plt.ylabel('Mean Gradient Variance')
plt.legend()
plt.grid(True, which="both", ls="--", c='0.7')
plt.show()

grouped_data = df_filtered.groupby('N')['mean_grad_variance']
mean_variance_by_N = grouped_data.mean().reset_index()
std_variance_by_N = grouped_data.std().reset_index()

display(mean_variance_by_N.head())
display(std_variance_by_N.head())

import matplotlib.ticker as ticker
import matplotlib.font_manager as fm
import scipy.stats
import numpy as np
from scipy.optimize import curve_fit
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.cm as cm

# Font property setup
font_prop = fm.FontProperties(fname='/content/drive/MyDrive/cmunbmr.ttf')  # replace with actual path

# Define the exponential function
def exponential_func(x, a, b):
    return a * np.exp(b * x)

# Function to limit data to N <= 20
def limit_to_N_20(df):
    return df[df['N'] <= 20]

# Ensure df_filtered_positive_grad is available
if 'df_filtered' not in locals() or df_filtered.empty:
    print("df_filtered DataFrame is not available or is empty. Please ensure it is filtered correctly.")
else:
    # --- Task 1: Plots for each distinct p value ---
    print("Generating plots for each individual p value (N <= 20)...")
    unique_p_values = df_filtered['p'].unique()

    for p_val in unique_p_values:
        print(f"Generating plot for p = {p_val}")

        # Filter data for the current p-value and limit to N <= 20
        df_p = df_filtered[df_filtered['p'] == p_val].copy()
        df_p = limit_to_N_20(df_p)

        if df_p.empty:
            print(f"No data for p = {p_val} with N <= 20. Skipping plot.")
            continue

        # Aggregate data for the current p-value
        grouped_data_p = df_p.groupby('N')['mean_grad_variance']
        mean_variance_by_N_p = grouped_data_p.mean().reset_index()
        std_variance_by_N_p = grouped_data_p.std().reset_index()

        # Ensure there is enough data for regression for this p-value (at least 2 points)
        if len(mean_variance_by_N_p) < 2:
            print(f"Not enough data points (N values) for regression for p = {p_val} with N <= 20. Skipping plot.")
            continue

        # Perform Exponential Fit for the current p-value
        x_data_p = mean_variance_by_N_p['N']
        y_data_p = mean_variance_by_N_p['mean_grad_variance']

        try:
            popt_p, pcov_p = curve_fit(exponential_func, x_data_p, y_data_p, maxfev=5000)
            A_p, B_p = popt_p
            # Calculate R-squared for Exponential Fit for this p-value
            residuals_p = y_data_p - exponential_func(x_data_p, A_p, B_p)
            ss_res_p = np.sum(residuals_p**2)
            ss_tot_p = np.sum((y_data_p - np.mean(y_data_p))**2)
            r_squared_p = 1 - (ss_res_p / ss_tot_p) if ss_tot_p > 0 else np.nan

        except RuntimeError:
            A_p, B_p = np.nan, np.nan
            r_squared_p = np.nan
            print(f"Exponential fit failed for p = {p_val}. Plotting without fit line.")

        # Calculate counts and proportions for coloring within this p-value
        counts_by_N_p = df_p['N'].value_counts().sort_index()
        total_count_p = len(df_p)
        proportions_by_N_p = counts_by_N_p / total_count_p * 100  # Percentage

        # Map proportions to the mean_variance_by_N_p DataFrame for plotting
        mean_variance_by_N_p['proportion'] = mean_variance_by_N_p['N'].map(proportions_by_N_p)

        # New plot formatting with colorbar, no title, no R² in label
        with plt.style.context('seaborn-v0_8-paper'):
            plt.figure(figsize=(10, 6))

            # Scatter plot with smaller points (s=120)
            scatter = plt.scatter(mean_variance_by_N_p['N'], mean_variance_by_N_p['mean_grad_variance'],
                                c=mean_variance_by_N_p['proportion'], cmap='viridis', s=120, alpha=0.7)

            # Add colorbar with standard font
            cbar = plt.colorbar(scatter)
            cbar.set_label('% of Total Data Points for p=' + str(p_val))
            cbar.ax.yaxis.label.set_fontproperties(font_prop)  # Use standard font
            cbar.ax.tick_params(labelsize=12)

            # Plot Exponential Fit if successful - removed R² from label
            if not np.isnan(A_p):
                x_fit_p = np.linspace(x_data_p.min(), x_data_p.max(), 200)
                y_fit_p = exponential_func(x_fit_p, A_p, B_p)
                plt.plot(x_fit_p, y_fit_p, linestyle='--', linewidth=3, color='red',
                        label=f'Fit: y = {A_p:.2e} * exp({B_p:.2f} * N)')

            # Add error bars for this p-value
            plt.errorbar(mean_variance_by_N_p['N'], mean_variance_by_N_p['mean_grad_variance'],
                        yerr=std_variance_by_N_p['mean_grad_variance'], fmt='o', capsize=12,
                        ecolor='black', elinewidth=1, color='none')

            plt.xlabel('N', fontsize=27, fontproperties=font_prop, labelpad=20)
            plt.ylabel('Mean Gradient Variance', fontsize=27, fontproperties=font_prop, labelpad=20)
            plt.grid(True, alpha=.25)
            plt.tick_params(axis='both', labelsize=12)

            # Set x-axis ticks to integers for this p-value
            plt.xticks(np.arange(min(mean_variance_by_N_p['N']), max(mean_variance_by_N_p['N'])+1, 1))

            # Calculate Pearson r for N vs log(Mean Variance) for this p-value
            y_data_p_log = np.log(y_data_p[y_data_p > 0])
            x_data_p_for_log_corr = x_data_p[y_data_p > 0]

            pearson_r_log_p = np.nan
            if len(x_data_p_for_log_corr) >= 2:
                pearson_r_log_p, _ = scipy.stats.pearsonr(x_data_p_for_log_corr, y_data_p_log)

            # No title as requested
            print(f"Pearson correlation coefficient (N vs log(Mean per N)) for p={p_val}: {pearson_r_log_p:.3f}")

            if not np.isnan(A_p):
                plt.legend()
            plt.show()

    # --- Task 2: Aggregated plot for p values 1, 3, 5, and 10 ---
    print("\nGenerating aggregated plot for p values 1, 3, 5, and 10 (N <= 20)...")
    selected_p_values = [1, 3, 5, 10]
    df_selected_p = df_filtered[df_filtered['p'].isin(selected_p_values)].copy()
    df_selected_p = limit_to_N_20(df_selected_p)

    if df_selected_p.empty:
        print(f"No data found for selected p values: {selected_p_values} with N <= 20. Skipping aggregated plot.")
    else:
        # Aggregate data for selected p values
        grouped_data_selected_p = df_selected_p.groupby('N')['mean_grad_variance']
        mean_variance_by_N_selected_p = grouped_data_selected_p.mean().reset_index()
        std_variance_by_N_selected_p = grouped_data_selected_p.std().reset_index()

        if len(mean_variance_by_N_selected_p) < 2:
            print(f"Not enough data points (N values) for regression for selected p values: {selected_p_values} with N <= 20. Skipping aggregated plot.")
        else:
            # Perform Exponential Fit for selected p values
            x_data_selected_p = mean_variance_by_N_selected_p['N']
            y_data_selected_p = mean_variance_by_N_selected_p['mean_grad_variance']

            try:
                popt_selected_p, pcov_selected_p = curve_fit(exponential_func, x_data_selected_p, y_data_selected_p, maxfev=5000)
                A_selected_p, B_selected_p = popt_selected_p
                residuals_selected_p = y_data_selected_p - exponential_func(x_data_selected_p, A_selected_p, B_selected_p)
                ss_res_selected_p = np.sum(residuals_selected_p**2)
                ss_tot_selected_p = np.sum((y_data_selected_p - np.mean(y_data_selected_p))**2)
                r_squared_selected_p = 1 - (ss_res_selected_p / ss_tot_selected_p) if ss_tot_selected_p > 0 else np.nan

            except RuntimeError:
                A_selected_p, B_selected_p = np.nan, np.nan
                r_squared_selected_p = np.nan
                print(f"Exponential fit failed for selected p values: {selected_p_values}. Plotting without fit line.")

            # Calculate counts and proportions for coloring within selected p values
            counts_by_N_selected_p = df_selected_p['N'].value_counts().sort_index()
            total_count_selected_p = len(df_selected_p)
            proportions_by_N_selected_p = counts_by_N_selected_p / total_count_selected_p * 100

            mean_variance_by_N_selected_p['proportion'] = mean_variance_by_N_selected_p['N'].map(proportions_by_N_selected_p)

            with plt.style.context('seaborn-v0_8-paper'):
                plt.figure(figsize=(10, 6))

                scatter_selected_p = plt.scatter(mean_variance_by_N_selected_p['N'], mean_variance_by_N_selected_p['mean_grad_variance'],
                                              c=mean_variance_by_N_selected_p['proportion'], cmap='viridis', s=120, alpha=0.7)

                cbar_selected_p = plt.colorbar(scatter_selected_p)
                cbar_selected_p.set_label(f'% of total data at N')
                cbar_selected_p.ax.yaxis.label.set_fontproperties(font_prop)  # Use standard font
                cbar_selected_p.ax.yaxis.label.set_fontsize(18)
                cbar_selected_p.ax.tick_params(labelsize=12)

                if not np.isnan(A_selected_p):
                    x_fit_selected_p = np.linspace(x_data_selected_p.min(), x_data_selected_p.max(), 200)
                    y_fit_selected_p = exponential_func(x_fit_selected_p, A_selected_p, B_selected_p)
                    plt.plot(x_fit_selected_p, y_fit_selected_p, linestyle='--', linewidth=3, color='red',
                            label=f'Fit: y = {A_selected_p:.2e} * exp({B_selected_p:.2f} * N)')

                plt.errorbar(mean_variance_by_N_selected_p['N'], mean_variance_by_N_selected_p['mean_grad_variance'],
                            yerr=std_variance_by_N_selected_p['mean_grad_variance'], fmt='o', capsize=12,
                            ecolor='black', elinewidth=1, color='none')

                plt.xlabel('N (Number of vertices)', fontsize=27, fontproperties=font_prop, labelpad=20)
                plt.ylabel('Mean Gradient Variance', fontsize=27, fontproperties=font_prop, labelpad=20)
                plt.grid(True, alpha=.25)
                plt.tick_params(axis='both', labelsize=12)

                plt.xticks(np.arange(min(mean_variance_by_N_selected_p['N']), max(mean_variance_by_N_selected_p['N'])+1, 1))

                # Calculate Pearson r for N vs log(Mean Variance) for selected p values
                y_data_selected_p_log = np.log(y_data_selected_p[y_data_selected_p > 0])
                x_data_selected_p_for_log_corr = x_data_selected_p[y_data_selected_p > 0]

                pearson_r_log_selected_p = np.nan
                if len(x_data_selected_p_for_log_corr) >= 2:
                    pearson_r_log_selected_p, _ = scipy.stats.pearsonr(x_data_selected_p_for_log_corr, y_data_selected_p_log)

                # No title as requested
                print(f"Pearson correlation coefficient (N vs log(Mean per N)) for p={selected_p_values}: {pearson_r_log_selected_p:.3f}")

                if not np.isnan(A_selected_p):
                    plt.legend()
                plt.show()

    # --- Task 3: Aggregated plot for the entire dataset ---
    print("\nGenerating aggregated plot for the entire dataset (N <= 20)...")
    df_all_p = df_filtered.copy()
    df_all_p = limit_to_N_20(df_all_p)

    if df_all_p.empty:
        print("No data found for the entire dataset with N <= 20. Skipping aggregated plot.")
    else:
        # Aggregate data for the entire dataset
        grouped_data_all_p = df_all_p.groupby('N')['mean_grad_variance']
        mean_variance_by_N_all_p = grouped_data_all_p.mean().reset_index()
        std_variance_by_N_all_p = grouped_data_all_p.std().reset_index()

        if len(mean_variance_by_N_all_p) < 2:
            print("Not enough data points (N values) for regression for the entire dataset with N <= 20. Skipping aggregated plot.")
        else:
            # Perform Exponential Fit for the entire dataset
            x_data_all_p = mean_variance_by_N_all_p['N']
            y_data_all_p = mean_variance_by_N_all_p['mean_grad_variance']

            try:
                popt_all_p, pcov_all_p = curve_fit(exponential_func, x_data_all_p, y_data_all_p, maxfev=5000)
                A_all_p, B_all_p = popt_all_p
                residuals_all_p = y_data_all_p - exponential_func(x_data_all_p, A_all_p, B_all_p)
                ss_res_all_p = np.sum(residuals_all_p**2)
                ss_tot_all_p = np.sum((y_data_all_p - np.mean(y_data_all_p))**2)
                r_squared_all_p = 1 - (ss_res_all_p / ss_tot_all_p) if ss_tot_all_p > 0 else np.nan

            except RuntimeError:
                A_all_p, B_all_p = np.nan, np.nan
                r_squared_all_p = np.nan
                print("Exponential fit failed for the entire dataset. Plotting without fit line.")

            # Calculate counts and proportions for coloring within the entire dataset
            counts_by_N_all_p = df_all_p['N'].value_counts().sort_index()
            total_count_all_p = len(df_all_p)
            proportions_by_N_all_p = counts_by_N_all_p / total_count_all_p * 100

            mean_variance_by_N_all_p['proportion'] = mean_variance_by_N_all_p['N'].map(proportions_by_N_all_p)

            with plt.style.context('seaborn-v0_8-paper'):
                plt.figure(figsize=(10, 6))

                scatter_all_p = plt.scatter(mean_variance_by_N_all_p['N'], mean_variance_by_N_all_p['mean_grad_variance'],
                                          c=mean_variance_by_N_all_p['proportion'], cmap='viridis', s=120, alpha=0.7)

                cbar_all_p = plt.colorbar(scatter_all_p)
                cbar_all_p.set_label('% of total points at N')
                cbar_all_p.ax.yaxis.label.set_fontproperties(font_prop)  # Use standard font
                cbar_all_p.ax.yaxis.label.set_fontsize(18)
                cbar_all_p.ax.tick_params(labelsize=12)

                if not np.isnan(A_all_p):
                    x_fit_all_p = np.linspace(x_data_all_p.min(), x_data_all_p.max(), 200)
                    y_fit_all_p = exponential_func(x_fit_all_p, A_all_p, B_all_p)
                    plt.plot(x_fit_all_p, y_fit_all_p, linestyle='--', linewidth=3, color='red',
                            label=f'Fit: y = {A_all_p:.2e} * exp({B_all_p:.2f} * N)')

                plt.errorbar(mean_variance_by_N_all_p['N'], mean_variance_by_N_all_p['mean_grad_variance'],
                            yerr=std_variance_by_N_all_p['mean_grad_variance'], fmt='o', capsize=12,
                            ecolor='black', elinewidth=1, color='none')

                plt.xlabel('N (Number of vertices)', fontsize=27, fontproperties=font_prop, labelpad=20)
                plt.ylabel('Mean Gradient Variance', fontsize=27, fontproperties=font_prop, labelpad=20)
                plt.grid(True, alpha=.25)
                plt.tick_params(axis='both', labelsize=12)

                plt.xticks(np.arange(min(mean_variance_by_N_all_p['N']), max(mean_variance_by_N_all_p['N'])+1, 1))

                # Calculate Pearson r for N vs log(Mean Variance) for the entire dataset
                y_data_all_p_log = np.log(y_data_all_p[y_data_all_p > 0])
                x_data_all_p_for_log_corr = x_data_all_p[y_data_all_p > 0]

                pearson_r_log_all_p = np.nan
                if len(x_data_all_p_for_log_corr) >= 2:
                    pearson_r_log_all_p, _ = scipy.stats.pearsonr(x_data_all_p_for_log_corr, y_data_all_p_log)

                # No title as requested
                print(f"Pearson correlation coefficient (N vs log(Mean per N)) for All p: {pearson_r_log_all_p:.3f}")

                if not np.isnan(A_all_p):
                    plt.legend()
                plt.show()

df_filtered_positive_grad = df[df['mean_grad_variance'] > 0].copy()
grouped_data_positive = df_filtered_positive_grad.groupby('N')['mean_grad_variance']
mean_variance_by_N_positive = grouped_data_positive.mean().reset_index() # Still needed for context, though not for regression target
std_variance_by_N_positive = grouped_data_positive.std().reset_index()

display(mean_variance_by_N_positive.head())
display(std_variance_by_N_positive.head())

std_variance = std_variance_by_N_positive['mean_grad_variance']
display(std_variance.head())

def exponential_func(x, a, b):
    return a * np.exp(b * x)

popt, pcov = curve_fit(exponential_func, std_variance_by_N_positive['N'], std_variance)

A, B = popt

import matplotlib.ticker as ticker # Import ticker
import scipy.stats # Import scipy.stats for Pearson r calculation
import numpy as np # Import numpy for log
from scipy.optimize import curve_fit # Import curve_fit

# Define the exponential function
def exponential_func(x, a, b):
    return a * np.exp(b * x)


# Ensure df_filtered_positive_grad is available
if 'df_filtered_positive_grad' not in locals() or df_filtered_positive_grad.empty:
    print("df_filtered_positive_grad DataFrame is not available or is empty. Please ensure it is filtered correctly.")
else:
    # --- Task 1: Plots for each distinct p value ---
    print("Generating plots for Standard Deviation for each individual p value...")
    unique_p_values = df_filtered_positive_grad['p'].unique()


    for p_val in unique_p_values:
        print(f"Generating plot for Standard Deviation for p = {p_val}")

        # Filter data for the current p-value
        df_p = df_filtered_positive_grad[df_filtered_positive_grad['p'] == p_val].copy()
        df_p = limit_to_N_20(df_p)
        if df_p.empty:
            print(f"No data for p = {p_val}. Skipping plot.")
            plt.title(f'Exponential Fit of Standard Deviation of Mean Gradient Variance vs N (Mean per N, p={p_val}), colored by Data Proportion\nNot enough data for fit')
            plt.xlabel('N')
            plt.ylabel('Standard Deviation of Mean Gradient Variance')
            plt.grid(True, which="both", ls="--", c='0.7')
            plt.show()
            continue

        # Aggregate data for the current p-value
        grouped_data_p = df_p.groupby('N')['mean_grad_variance']
        # We still need mean_variance_by_N_p for proportion calculation if needed, but not for plotting/fitting y
        mean_variance_by_N_p = grouped_data_p.mean().reset_index()
        std_variance_by_N_p = grouped_data_p.std().reset_index().dropna(subset=['mean_grad_variance']) # Drop NaNs in std

        # Ensure there is enough data for regression for this p-value (at least 2 points with non-NaN std)
        if len(std_variance_by_N_p) < 2:
            print(f"Not enough data points (N values with non-NaN Standard Deviation) for regression for p = {p_val}. Skipping plot.")
            plt.title(f'Exponential Fit of Standard Deviation of Mean Gradient Variance vs N (Mean per N, p={p_val}), colored by Data Proportion\nNot enough data for fit')
            plt.xlabel('N')
            plt.ylabel('Standard Deviation of Mean Gradient Variance')
            plt.grid(True, which="both", ls="--", c='0.7')
            plt.show()
            continue


        # Perform Exponential Fit for the current p-value (using Standard Deviation as y)
        x_data_p = std_variance_by_N_p['N']
        y_data_p = std_variance_by_N_p['mean_grad_variance'] # Using Standard Deviation here

        try:
            # Removed p0=initial_guess
            popt_p, pcov_p = curve_fit(exponential_func, x_data_p, y_data_p, maxfev=5000)
            A_p, B_p = popt_p
            # Calculate R-squared for Exponential Fit for this p-value
            residuals_p = y_data_p - exponential_func(x_data_p, A_p, B_p)
            ss_res_p = np.sum(residuals_p**2)
            ss_tot_p = np.sum((y_data_p - np.mean(y_data_p))**2)
            r_squared_p = 1 - (ss_res_p / ss_tot_p) if ss_tot_p > 0 else np.nan

        except RuntimeError:
            A_p, B_p = np.nan, np.nan
            r_squared_p = np.nan
            print(f"Exponential fit failed for p = {p_val}. Plotting without fit line.")

        # Calculate counts and proportions for coloring within this p-value
        # Use counts from the original df_p before dropping NaNs in std
        counts_by_N_p = df_p['N'].value_counts().sort_index()
        total_count_p = len(df_p)
        proportions_by_N_p = counts_by_N_p / total_count_p * 100 # Percentage

        # Map proportions to the std_variance_by_N_p DataFrame for plotting
        std_variance_by_N_p['proportion'] = std_variance_by_N_p['N'].map(proportions_by_N_p)


        plt.figure(figsize=(10, 6))

        # Scatter plot with coloring by proportion for this p-value (using Standard Deviation as y)
        try:
            norm_p = plt.Normalize(std_variance_by_N_p['proportion'].min(), std_variance_by_N_p['proportion'].max())
        except ValueError:
            norm_p = plt.Normalize(0, 1) # Fallback norm if range is zero

        cmap = cm.viridis

        scatter = plt.scatter(std_variance_by_N_p['N'], std_variance_by_N_p['mean_grad_variance'], # Plotting Standard Deviation
                              c=std_variance_by_N_p['proportion'], cmap=cmap, norm=norm_p,
                              label=f'Standard Deviation of Mean Gradient Variance per N (p={p_val})', alpha=0.6, s=70)

        # Add colorbar for this p-value
        cbar = plt.colorbar(scatter)
        cbar.set_label('% of Total Data Points for p=' + str(p_val))


        # Plot Exponential Fit if successful for this p-value
        if not np.isnan(A_p):
            x_fit_p = np.linspace(x_data_p.min(), x_data_p.max(), 200)
            y_fit_p = exponential_func(x_fit_p, A_p, B_p)
            plt.plot(x_fit_p, y_fit_p, color='red', label=f'Fit: y = {A_p:.2e} * exp({B_p:.2f} * N), R²={r_squared_p:.2f}')

        # Removed error bars


        # Set x-axis ticks to integers for this p-value
        plt.xticks(np.arange(min(std_variance_by_N_p['N']), max(std_variance_by_N_p['N'])+1, 1))

        # Calculate Pearson r for N vs log(Standard Deviation) for this p-value
        # Ensure log is taken only for positive standard deviation values
        y_data_p_log = np.log(y_data_p[y_data_p > 0])
        x_data_p_for_log_corr = x_data_p[y_data_p > 0]

        pearson_r_log_p = np.nan # Default to NaN
        if len(x_data_p_for_log_corr) >= 2:
             pearson_r_log_p, _ = scipy.stats.pearsonr(x_data_p_for_log_corr, y_data_p_log)


        if len(std_variance_by_N_p) >= 2:
            # Display Pearson r for N vs log(Standard Deviation) in title
            plt.title(f'Exponential Fit of Standard Deviation of Mean Gradient Variance vs N (Mean per N, p={p_val}), colored by Data Proportion')
            print(f"Pearson correlation coefficient (N vs log(Std per N)) for p={p_val}: {pearson_r_log_p:.3f}")
        else:
            plt.title(f'Exponential Fit of Standard Deviation of Mean Gradient Variance vs N (Mean per N, p={p_val}), colored by Data Proportion')
            print(f"Not enough data points (N values) to calculate Pearson r for p={p_val}")


        plt.xlabel('N')
        plt.ylabel('Standard Deviation of Mean Gradient Variance')
        plt.legend()
        plt.grid(True, which="both", ls="--", c='0.7')
        plt.show()

    # --- Task 2: Aggregated plot for p values 1, 3, 5, and 10 ---
    print("\nGenerating aggregated plot for Standard Deviation for p values 1, 3, 5, and 10...")
    selected_p_values = [1, 3, 5, 10]
    df_selected_p = df_filtered_positive_grad[df_filtered_positive_grad['p'].isin(selected_p_values)].copy()
    df_selected_p = limit_to_N_20(df_selected_p)
    if df_selected_p.empty:
         print(f"No data found for selected p values: {selected_p_values}. Skipping aggregated plot.")
    else:
        # Aggregate data for selected p values
        grouped_data_selected_p = df_selected_p.groupby('N')['mean_grad_variance']
        mean_variance_by_N_selected_p = grouped_data_selected_p.mean().reset_index() # Still needed for proportion calculation
        std_variance_by_N_selected_p = grouped_data_selected_p.std().reset_index().dropna(subset=['mean_grad_variance']) # Drop NaNs in std

        if len(std_variance_by_N_selected_p) < 2:
            print(f"Not enough data points (N values with non-NaN Standard Deviation) for regression for selected p values: {selected_p_values}. Skipping aggregated plot.")
        else:
            # Perform Exponential Fit for selected p values (using Standard Deviation as y)
            x_data_selected_p = std_variance_by_N_selected_p['N']
            y_data_selected_p = std_variance_by_N_selected_p['mean_grad_variance'] # Using Standard Deviation here

            try:
                # Removed p0=initial_guess
                popt_selected_p, pcov_selected_p = curve_fit(exponential_func, x_data_selected_p, y_data_selected_p, maxfev=5000)
                A_selected_p, B_selected_p = popt_selected_p
                 # Calculate R-squared for Exponential Fit for selected p values
                residuals_selected_p = y_data_selected_p - exponential_func(x_data_selected_p, A_selected_p, B_selected_p)
                ss_res_selected_p = np.sum(residuals_selected_p**2)
                ss_tot_selected_p = np.sum((y_data_selected_p - np.mean(y_data_selected_p))**2)
                r_squared_selected_p = 1 - (ss_res_selected_p / ss_tot_selected_p) if ss_tot_selected_p > 0 else np.nan

            except RuntimeError:
                 A_selected_p, B_selected_p = np.nan, np.nan
                 r_squared_selected_p = np.nan
                 print(f"Exponential fit failed for selected p values: {selected_p_values}. Plotting without fit line.")

            # Calculate counts and proportions for coloring within selected p values
            counts_by_N_selected_p = df_selected_p['N'].value_counts().sort_index()
            total_count_selected_p = len(df_selected_p)
            proportions_by_N_selected_p = counts_by_N_selected_p / total_count_selected_p * 100 # Percentage

            # Map proportions to the std_variance_by_N_selected_p DataFrame for plotting
            std_variance_by_N_selected_p['proportion'] = std_variance_by_N_selected_p['N'].map(proportions_by_N_selected_p)

            plt.figure(figsize=(10, 6))

            # Scatter plot with coloring by proportion for selected p values (using Standard Deviation as y)
            try:
                norm_selected_p = plt.Normalize(std_variance_by_N_selected_p['proportion'].min(), std_variance_by_N_selected_p['proportion'].max())
            except ValueError:
                norm_selected_p = plt.Normalize(0, 1)

            cmap = cm.viridis

            scatter_selected_p = plt.scatter(std_variance_by_N_selected_p['N'], std_variance_by_N_selected_p['mean_grad_variance'], # Plotting Standard Deviation
                                          c=std_variance_by_N_selected_p['proportion'], cmap=cmap, norm=norm_selected_p,
                                          label=f'Standard Deviation of Mean Gradient Variance per N (p={selected_p_values})', alpha=0.6, s=70)

            cbar_selected_p = plt.colorbar(scatter_selected_p)
            cbar_selected_p.set_label(f'% of Total Data Points for p={selected_p_values}')

            if not np.isnan(A_selected_p):
                x_fit_selected_p = np.linspace(x_data_selected_p.min(), x_data_selected_p.max(), 200)
                y_fit_selected_p = exponential_func(x_fit_selected_p, A_selected_p, B_selected_p)
                plt.plot(x_fit_selected_p, y_fit_selected_p, color='red', label=f'Fit: y = {A_selected_p:.2e} * exp({B_selected_p:.2f} * N), R²={r_squared_selected_p:.2f}')

            # Removed error bars


            plt.xticks(np.arange(min(std_variance_by_N_selected_p['N']), max(std_variance_by_N_selected_p['N'])+1, 1))

            # Calculate Pearson r for N vs log(Standard Deviation) for selected p values
            y_data_selected_p_log = np.log(y_data_selected_p[y_data_selected_p > 0])
            x_data_selected_p_for_log_corr = x_data_selected_p[y_data_selected_p > 0]

            pearson_r_log_selected_p = np.nan # Default to NaN
            if len(x_data_selected_p_for_log_corr) >= 2:
                 pearson_r_log_selected_p, _ = scipy.stats.pearsonr(x_data_selected_p_for_log_corr, y_data_selected_p_log)


            if len(std_variance_by_N_selected_p) >= 2:
                # Display Pearson r for N vs log(Standard Deviation) in title
                plt.title(f'Exponential Fit of Standard Deviation of Mean Gradient Variance vs N (Mean per N, p={selected_p_values}), colored by Data Proportion')
                print(f"Pearson correlation coefficient (N vs log(Std per N)) for p={selected_p_values}: {pearson_r_log_selected_p:.3f}")
            else:
                 plt.title(f'Exponential Fit of Standard Deviation of Mean Gradient Variance vs N (Mean per N, p={selected_p_values}), colored by Data Proportion')
                 print(f"Not enough data points (N values) to calculate Pearson r for p={selected_p_values}")


            plt.xlabel('N')
            plt.ylabel('Standard Deviation of Mean Gradient Variance')
            plt.legend()
            plt.grid(True, which="both", ls="--", c='0.7')
            plt.show()

    # --- Task 3: Aggregated plot for the entire dataset ---
    print("\nGenerating aggregated plot for Standard Deviation for the entire dataset...")
    df_all_p = df_filtered_positive_grad.copy() # Use the entire filtered dataset

    if df_all_p.empty:
         print("No data found for the entire dataset. Skipping aggregated plot.")
    else:
        # Aggregate data for the entire dataset
        grouped_data_all_p = df_all_p.groupby('N')['mean_grad_variance']
        mean_variance_by_N_all_p = grouped_data_all_p.mean().reset_index() # Still needed for proportion calculation
        std_variance_by_N_all_p = grouped_data_all_p.std().reset_index().dropna(subset=['mean_grad_variance']) # Drop NaNs in std

        if len(std_variance_by_N_all_p) < 2:
            print("Not enough data points (N values with non-NaN Standard Deviation) for regression for the entire dataset. Skipping aggregated plot.")
        else:
            # Perform Exponential Fit for the entire dataset (using Standard Deviation as y)
            x_data_all_p = std_variance_by_N_all_p['N']
            y_data_all_p = std_variance_by_N_all_p['mean_grad_variance'] # Using Standard Deviation here

            try:
                # Removed p0=initial_guess
                popt_all_p, pcov_all_p = curve_fit(exponential_func, x_data_all_p, y_data_all_p, maxfev=5000)
                A_all_p, B_all_p = popt_all_p
                 # Calculate R-squared for Exponential Fit for the entire dataset
                residuals_all_p = y_data_all_p - exponential_func(x_data_all_p, A_all_p, B_all_p)
                ss_res_all_p = np.sum(residuals_all_p**2)
                ss_tot_all_p = np.sum((y_data_all_p - np.mean(y_data_all_p))**2)
                r_squared_all_p = 1 - (ss_res_all_p / ss_tot_all_p) if ss_tot_all_p > 0 else np.nan

            except RuntimeError:
                 A_all_p, B_all_p = np.nan, np.nan
                 r_squared_all_p = np.nan
                 print("Exponential fit failed for the entire dataset. Plotting without fit line.")

            # Calculate counts and proportions for coloring within the entire dataset
            counts_by_N_all_p = df_all_p['N'].value_counts().sort_index()
            total_count_all_p = len(df_all_p)
            proportions_by_N_all_p = counts_by_N_all_p / total_count_all_p * 100 # Percentage

            # Map proportions to the std_variance_by_N_all_p DataFrame for plotting
            std_variance_by_N_all_p['proportion'] = std_variance_by_N_all_p['N'].map(proportions_by_N_all_p)


            plt.figure(figsize=(10, 6))

            # Scatter plot with coloring by proportion for the entire dataset (using Standard Deviation as y)
            try:
                norm_all_p = plt.Normalize(std_variance_by_N_all_p['proportion'].min(), std_variance_by_N_all_p['proportion'].max())
            except ValueError:
                norm_all_p = plt.Normalize(0, 1)

            cmap = cm.viridis

            scatter_all_p = plt.scatter(std_variance_by_N_all_p['N'], std_variance_by_N_all_p['mean_grad_variance'], # Plotting Standard Deviation
                                     c=std_variance_by_N_all_p['proportion'], cmap=cmap, norm=norm_all_p,
                                     label='Standard Deviation of Mean Gradient Variance per N (All p)', alpha=0.6, s=70)

            cbar_all_p = plt.colorbar(scatter_all_p)
            cbar_all_p.set_label('% of Total Data Points (All p)')


            if not np.isnan(A_all_p):
                x_fit_all_p = np.linspace(x_data_all_p.min(), x_data_all_p.max(), 200)
                y_fit_all_p = exponential_func(x_fit_all_p, A_all_p, B_all_p)
                plt.plot(x_fit_all_p, y_fit_all_p, color='red', label=f'Fit: y = {A_all_p:.2e} * exp({B_all_p:.2f} * N), R²={r_squared_all_p:.2f}')

            # Removed error bars


            plt.xticks(np.arange(min(std_variance_by_N_all_p['N']), max(std_variance_by_N_all_p['N'])+1, 1))

            # Calculate Pearson r for N vs log(Standard Deviation) for the entire dataset
            y_data_all_p_log = np.log(y_data_all_p[y_data_all_p > 0])
            x_data_all_p_for_log_corr = x_data_all_p[y_data_all_p > 0]

            pearson_r_log_all_p = np.nan # Default to NaN
            if len(x_data_all_p_for_log_corr) >= 2:
                 pearson_r_log_all_p, _ = scipy.stats.pearsonr(x_data_all_p_for_log_corr, y_data_all_p_log)


            if len(std_variance_by_N_all_p) >= 2:
                # Display Pearson r for N vs log(Standard Deviation) in title
                plt.title(f'Exponential Fit of Standard Deviation of Mean Gradient Variance vs N (Mean per N, All p), colored by Data Proportion')
                print(f"Pearson correlation coefficient (N vs log(Std per N)) for All p: {pearson_r_log_all_p:.3f}")
            else:
                 plt.title(f'Exponential Fit of Standard Deviation of Mean Gradient Variance vs N (Mean per N, All p), colored by Data Proportion')
                 print("Not enough data points (N values) to calculate Pearson r for All p")


            plt.xlabel('N')
            plt.ylabel('Standard Deviation of Mean Gradient Variance')
            plt.legend()
            plt.grid(True, which="both", ls="--", c='0.7')
            plt.show()



"""# Task
For every distinct value of p, for each of the clusters, compute the average and standard deviation of their approximation ratio, number of iterations, and number of function calls, and then plot each of the averages against the pearson r, including error bars. On each of the plots, display the pearson correlations of each of those plots both for an exponential and linear fit along with the relevant parameters.

## Prepare data

### Subtask:
Ensure the `corr_feat_df_vals` DataFrame is available and contains the necessary columns ('p', 'feature', 'cluster_label', 'pearson_r_log', 'adjs'). Ensure the original `df` DataFrame contains the metrics to analyze ('approximation ratio', 'number of iterations', 'number of function calls', 'adj', 'p').

**Reasoning**:
Verify the existence and required columns of `corr_feat_df_vals` and `df` DataFrames.
"""

print("Checking corr_feat_df_vals columns:")
print(corr_feat_df_vals.columns)

print("\nChecking df columns:")
print(df.columns)

required_corr_cols = ['p', 'feature', 'cluster_label', 'pearson_r_log', 'adjs']
missing_corr_cols = [col for col in required_corr_cols if col not in corr_feat_df_vals.columns]

if missing_corr_cols:
    print(f"\ncorr_feat_df_vals is missing the following columns: {missing_corr_cols}")
else:
    print("\ncorr_feat_df_vals contains all required columns.")

required_df_cols = ['approximation ratio', 'number of iterations', 'number of function calls', 'adj', 'p']
missing_df_cols = [col for col in required_df_cols if col not in df.columns]

if missing_df_cols:
    print(f"\ndf is missing the following columns: {missing_df_cols}")
else:
    print("\ndf contains all required columns.")

"""**Reasoning**:
The check shows that `df` is missing the columns 'approximation ratio', 'number of iterations', and 'number of function calls'. I need to check the column names in the original `df` to see if they exist under different names. Based on the output of `df.columns`, the relevant columns appear to be 'approx', 'nit', and 'f_calls'. I will verify these column names and ensure they are present.


"""

required_df_cols_corrected = ['approx', 'nit', 'f_calls', 'adj', 'p']
missing_df_cols_corrected = [col for col in required_df_cols_corrected if col not in df.columns]

if missing_df_cols_corrected:
    print(f"\ndf is missing the following corrected columns: {missing_df_cols_corrected}")
else:
    print("\ndf contains all required corrected columns.")

"""## Extract metric values for clusters

### Subtask:
Extract the values of 'approx', 'nit', and 'f_calls' from the `df` DataFrame for each cluster defined in `corr_feat_df_vals`, matching by adjacency matrix and p-value. Store these lists of values within the `corr_feat_df_vals` DataFrame.

**Reasoning**:
Define a function to extract metric values from the main DataFrame for each cluster and then call it to get the required data.
"""

def extract_cluster_metrics(df, corr_feat_df, metric_columns=['approx', 'nit', 'f_calls']):
    """
    Extracts values from specified metric columns from the main DataFrame for each cluster
    defined in corr_feat_df, matching by adjacency matrix and p-value.

    Args:
        df (pd.DataFrame): The main DataFrame containing 'adj', 'p', and metric columns.
        corr_feat_df (pd.DataFrame): DataFrame with cluster information, including 'adjs' and 'p'.
        metric_columns (list): A list of column names in df to extract values from.

    Returns:
        pd.DataFrame: A copy of corr_feat_df with added columns for each metric,
                      containing lists of values for each cluster.
                      Returns an empty DataFrame if no clusters are in corr_feat_df or
                      if any metric column is not found in df.
    """
    if corr_feat_df.empty:
        print("Input corr_feat_df is empty.")
        return pd.DataFrame()

    for col in metric_columns:
        if col not in df.columns:
            print(f"Error: Metric column '{col}' not found in the main DataFrame.")
            return pd.DataFrame()

    # Hash adjacency matrices in df for fast lookup
    df_hashed = df.copy()
    df_hashed['adj_hash'] = df_hashed['adj'].apply(hash_adj)

    # Build lookup: (adj_hash, p) -> list of indices in df_hashed
    adj_p_index_lookup = defaultdict(list)
    for idx, row in df_hashed.iterrows():
        key = (row['adj_hash'], row['p'])
        adj_p_index_lookup[key].append(idx)

    # Add metric columns to corr_feat_df
    corr_df_with_metrics = corr_feat_df.copy()
    metric_value_lists = {metric: [] for metric in metric_columns}

    for idx, cluster_row in corr_df_with_metrics.iterrows():
        p_val_cluster = cluster_row['p']
        cluster_adjs = cluster_row['adjs']
        cluster_adj_hashes = set(hash_adj(adj) for adj in cluster_adjs)

        # Gather all matching metric values for this cluster and p
        cluster_metric_vals = {metric: [] for metric in metric_columns}

        for adj_hash in cluster_adj_hashes:
            matching_indices = adj_p_index_lookup.get((adj_hash, p_val_cluster), [])
            for df_idx in matching_indices:
                 row_data = df.loc[df_idx]
                 for metric in metric_columns:
                    if metric in row_data:
                        cluster_metric_vals[metric].append(row_data[metric])

        for metric in metric_columns:
            metric_value_lists[metric].append(cluster_metric_vals[metric])

    for metric in metric_columns:
        corr_df_with_metrics[f'{metric}_values'] = metric_value_lists[metric]


    return corr_df_with_metrics

# Call the function to extract the metric values
corr_feat_df_metrics = extract_cluster_metrics(df, corr_feat_df_vals)

display(corr_feat_df_metrics.head())

"""## Compute cluster metrics

### Subtask:
For each cluster in the modified `corr_feat_df_metrics`, compute the average and standard deviation for each of the three metrics ('approx', 'nit', 'f_calls') based on the extracted values. Store these averages and standard deviations in new columns in the `corr_feat_df_metrics` DataFrame.

**Reasoning**:
Calculate the mean and standard deviation for 'approx', 'nit', and 'f_calls' for each cluster and store them in new columns in `corr_feat_df_metrics`.
"""

def calculate_cluster_stats(corr_df, metric_columns=['approx', 'nit', 'f_calls']):
    """
    Calculates the mean and standard deviation for specified metric value lists
    within each row of a DataFrame.

    Args:
        corr_df (pd.DataFrame): DataFrame with columns containing lists of metric values
                                (e.g., 'approx_values', 'nit_values').
        metric_columns (list): A list of metric names for which '_values' columns exist.

    Returns:
        pd.DataFrame: The input DataFrame with new columns for mean and standard deviation
                      for each metric (e.g., 'approx_mean', 'approx_std').
    """
    df_with_stats = corr_df.copy()

    for metric in metric_columns:
        values_col = f'{metric}_values'
        mean_col = f'{metric}_mean'
        std_col = f'{metric}_std'

        if values_col in df_with_stats.columns:
            df_with_stats[mean_col] = df_with_stats[values_col].apply(
                lambda x: np.mean(x) if x else np.nan
            )
            df_with_stats[std_col] = df_with_stats[values_col].apply(
                lambda x: np.std(x) if len(x) > 1 else (0 if x else np.nan)
            )
        else:
            print(f"Warning: Column '{values_col}' not found in DataFrame.")

    return df_with_stats

# Calculate the mean and standard deviation for the extracted metrics
corr_feat_df_metrics = calculate_cluster_stats(corr_feat_df_metrics)

# Display the head of the updated DataFrame
display(corr_feat_df_metrics.head())

"""## Iterate through p values and metrics

### Subtask:
Loop through each distinct value of 'p' and each of the three metrics.

**Reasoning**:
Get the unique 'p' values and the list of metrics, then start the nested loops as per the instructions.
"""

unique_p_values = corr_feat_df_metrics['p'].unique()
metrics_to_plot = ['approx', 'nit', 'f_calls']

for p_val in unique_p_values:
    for metric in metrics_to_plot:
        print(f"Processing p={p_val}, Metric={metric}")

"""**Reasoning**:
The nested loops are set up. Now, filter the DataFrame for the current p-value and metric, extract the relevant data for plotting, and perform the linear and exponential fits.


"""

def exponential_func(x, a, b):
    return a * np.exp(b * x)

def linear_func(x, m, c):
    return m * x + c

for p_val in unique_p_values:
    for metric in metrics_to_plot:
        print(f"Processing p={p_val}, Metric={metric}")

        # Filter data for the current p-value and metric
        metric_mean_col = f'{metric}_mean'
        metric_std_col = f'{metric}_std'

        # Filter out rows where the mean metric value is NaN (no data for the cluster)
        filtered_group = corr_feat_df_metrics[
            (corr_feat_df_metrics['p'] == p_val) &
            corr_feat_df_metrics[metric_mean_col].notna()
        ].copy()

        if filtered_group.empty:
            print(f"  No valid data for p={p_val}, Metric={metric}. Skipping plotting and fitting.")
            continue

        # Extract data for plotting (Pearson r vs Metric Mean)
        x_plot_data = filtered_group['pearson_r_log'].values
        y_plot_data = filtered_group[metric_mean_col].values
        y_error_data = filtered_group[metric_std_col].values # Use standard deviation for error bars

        # Perform Exponential Fit
        try:
            popt_exp, pcov_exp = curve_fit(exponential_func, x_plot_data, y_plot_data, maxfev=5000)
            A_exp, B_exp = popt_exp
            # Calculate R-squared for Exponential Fit
            residuals_exp = y_plot_data - exponential_func(x_plot_data, A_exp, B_exp)
            ss_res_exp = np.sum(residuals_exp**2)
            ss_tot_exp = np.sum((y_plot_data - np.mean(y_plot_data))**2)
            r_squared_exp = 1 - (ss_res_exp / ss_tot_exp) if ss_tot_exp > 0 else np.nan

        except RuntimeError:
            A_exp, B_exp = np.nan, np.nan
            r_squared_exp = np.nan
            print(f"  Exponential fit failed for p={p_val}, Metric={metric}.")


        # Perform Linear Fit
        try:
            slope_linear, intercept_linear, r_linear, p_value_linear, std_err_linear = linregress(x_plot_data, y_plot_data)
            # R-squared for Linear Fit is simply r_linear**2
            r_squared_linear = r_linear**2

        except ValueError:
            slope_linear, intercept_linear, r_linear, r_squared_linear = np.nan, np.nan, np.nan, np.nan
            print(f"  Linear fit failed for p={p_val}, Metric={metric}.")

"""**Reasoning**:
The fitting is done. Now, plot the results for each p-value and metric, including the scatter points with error bars and the fitted curves with their parameters and R-squared values in the title or legend.


"""

for p_val in unique_p_values:
    for metric in metrics_to_plot:
        print(f"Plotting p={p_val}, Metric={metric}")

        # Filter data for the current p-value and metric (re-filtering to ensure all variables are available)
        metric_mean_col = f'{metric}_mean'
        metric_std_col = f'{metric}_std'

        filtered_group = corr_feat_df_metrics[
            (corr_feat_df_metrics['p'] == p_val) &
            corr_feat_df_metrics[metric_mean_col].notna()
        ].copy()

        if filtered_group.empty:
            print(f"  No valid data for p={p_val}, Metric={metric}. Skipping plotting.")
            continue

        # Extract data for plotting (Pearson r vs Metric Mean)
        x_plot_data = filtered_group['pearson_r_log'].values
        y_plot_data = filtered_group[metric_mean_col].values
        y_error_data = filtered_group[metric_std_col].values # Use standard deviation for error bars

        # Perform Exponential Fit (re-fitting to ensure parameters are available for plotting)
        try:
            popt_exp, pcov_exp = curve_fit(exponential_func, x_plot_data, y_plot_data, maxfev=5000)
            A_exp, B_exp = popt_exp
            # Calculate R-squared for Exponential Fit
            residuals_exp = y_plot_data - exponential_func(x_plot_data, A_exp, B_exp)
            ss_res_exp = np.sum(residuals_exp**2)
            ss_tot_exp = np.sum((y_plot_data - np.mean(y_plot_data))**2)
            r_squared_exp = 1 - (ss_res_exp / ss_tot_exp) if ss_tot_exp > 0 else np.nan

        except RuntimeError:
            A_exp, B_exp = np.nan, np.nan
            r_squared_exp = np.nan
            print(f"  Exponential fit failed for p={p_val}, Metric={metric}. Plotting without exponential fit.")


        # Perform Linear Fit (re-fitting to ensure parameters are available for plotting)
        try:
            slope_linear, intercept_linear, r_linear, p_value_linear, std_err_linear = linregress(x_plot_data, y_plot_data)
            # R-squared for Linear Fit is simply r_linear**2
            r_squared_linear = r_linear**2

        except ValueError:
            slope_linear, intercept_linear, r_linear, r_squared_linear = np.nan, np.nan, np.nan, np.nan
            print(f"  Linear fit failed for p={p_val}, Metric={metric}. Plotting without linear fit.")


        # Create the plot
        plt.figure(figsize=(10, 6))

        # Scatter plot with error bars
        plt.errorbar(x_plot_data, y_plot_data, yerr=y_error_data, fmt='o', ecolor='gray', capsize=3, alpha=0.6, label='Cluster Data (Mean +/- Std Dev)')

        # Plot Exponential Fit if successful
        if not np.isnan(A_exp):
            x_fit_exp = np.linspace(x_plot_data.min(), x_plot_data.max(), 100)
            y_fit_exp = exponential_func(x_fit_exp, A_exp, B_exp)
            plt.plot(x_fit_exp, y_fit_exp, color='red', linestyle='--', label=f'Exp Fit: y={A_exp:.2e}*exp({B_exp:.2f}*x), R²={r_squared_exp:.2f}')

        # Plot Linear Fit if successful
        if not np.isnan(slope_linear):
            x_fit_linear = np.linspace(x_plot_data.min(), x_plot_data.max(), 100)
            y_fit_linear = linear_func(x_fit_linear, slope_linear, intercept_linear)
            plt.plot(x_fit_linear, y_fit_linear, color='green', linestyle='-', label=f'Linear Fit: y={slope_linear:.2f}*x+{intercept_linear:.2f}, R²={r_squared_linear:.2f}')


        plt.xlabel('Pearson r (Mean Grad Variance vs N)')
        plt.ylabel(f'Mean {metric.replace("_", " ").title()}')
        plt.title(f'Mean {metric.replace("_", " ").title()} vs Pearson r (Mean Grad Variance vs N) for p={p_val}')
        plt.legend()
        plt.grid(True, which="both", ls="--", c='0.7')
        plt.show()

import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
import numpy as np

# Font property setup (optional for histograms)
font_prop = fm.FontProperties(fname='/content/drive/MyDrive/cmunbmr.ttf')  # replace with actual path

# Function to limit data to N <= 20
def limit_to_N_20(df):
    if 'N' in df.columns:
        return df[df['N'] <= 20]
    else:
        print("Column 'N' not found in DataFrame; using all data")
        return df

# Ensure corr_feat_df_vals is available and contains 'p' and 'pearson_r_log'
if 'corr_feat_df_vals' not in locals() or corr_feat_df_vals.empty:
    print("corr_feat_df_vals DataFrame is not available or is empty. Please ensure it is loaded and processed.")
else:
    # Filter data to N <= 20
    corr_feat_df_vals_filtered = limit_to_N_20(corr_feat_df_vals)

    # Define common bin edges for consistent histograms
    num_bins = 30
    common_bins = np.linspace(-1, 1, num_bins + 1)

    # --- Task 1: Plot histograms for each distinct p value ---
    print("Generating histograms for each individual p value (N <= 20)...")
    unique_p_values_all = sorted(corr_feat_df_vals_filtered['p'].unique())

    for p_val in unique_p_values_all:
        # Filter data for the current p-value
        pearson_r_values = corr_feat_df_vals_filtered[corr_feat_df_vals_filtered['p'] == p_val]['pearson_r_log'].dropna()

        if pearson_r_values.empty:
            print(f"No Pearson r values found for p={p_val} with N <= 20. Skipping histogram.")
            continue

        # New formatting style
        with plt.style.context('seaborn-v0_8-paper'):
            plt.figure(figsize=(10, 6))
            sns.histplot(pearson_r_values, bins=common_bins, kde=True, color='steelblue')
            # No title as per the new style
            plt.xlabel('Pearson r (Mean Gradient Variance vs N)', fontsize=27, fontproperties=font_prop, labelpad=20)
            plt.ylabel('Frequency', fontsize=27, fontproperties=font_prop, labelpad=20)
            plt.xlim(-1, 1)
            plt.grid(True, alpha=.25)
            plt.tick_params(axis='both', labelsize=12)
            plt.show()

    # --- Task 2: Plot aggregated histogram for p values 1, 3, 5, and 10 ---
    print("\nGenerating aggregated histogram for p values 1, 3, 5, and 10 (N <= 20)...")
    selected_p_values = [1, 3, 5, 10]

    # Filter the DataFrame for the selected p values
    filtered_corr_df = corr_feat_df_vals_filtered[corr_feat_df_vals_filtered['p'].isin(selected_p_values)].copy()
    all_pearson_r_values_filtered = filtered_corr_df['pearson_r_log'].dropna()

    if all_pearson_r_values_filtered.empty:
        print(f"No Pearson r values found across the selected p values: {selected_p_values} with N <= 20 for aggregated histogram.")
    else:
        with plt.style.context('seaborn-v0_8-paper'):
            plt.figure(figsize=(10, 6))
            sns.histplot(all_pearson_r_values_filtered, bins=common_bins, kde=True, color='steelblue')
            # No title as per the new style
            plt.xlabel('Pearson r (Mean Gradient Variance vs N)', fontsize=27, fontproperties=font_prop, labelpad=20)
            plt.ylabel('Frequency', fontsize=27, fontproperties=font_prop, labelpad=20)
            plt.xlim(-1, 1)
            plt.grid(True, alpha=.25)
            plt.tick_params(axis='both', labelsize=12)
            plt.show()

    # --- Task 3: Plot aggregated histogram for the entire dataset ---
    print("\nGenerating aggregated histogram for the entire dataset (N <= 20)...")
    all_pearson_r_values_total = corr_feat_df_vals_filtered['pearson_r_log'].dropna()

    if all_pearson_r_values_total.empty:
        print("No Pearson r values found across the entire dataset with N <= 20 for aggregated histogram.")
    else:
        with plt.style.context('seaborn-v0_8-paper'):
            plt.figure(figsize=(10, 6))
            sns.histplot(all_pearson_r_values_total, bins=common_bins, kde=True, color='steelblue')
            # No title as per the new style
            plt.xlabel('Pearson r (Mean Gradient Variance vs N)', fontsize=27, fontproperties=font_prop, labelpad=20)
            plt.ylabel('Frequency', fontsize=27, fontproperties=font_prop, labelpad=20)
            plt.xlim(-1, 1)
            plt.grid(True, alpha=.25)
            plt.tick_params(axis='both', labelsize=12)
            plt.show()

import pandas as pd
import numpy as np

def compute_and_rank_pearson_stats(corr_feat_df, p_values=[1, 3, 5, 10]):
    """
    Computes average and standard deviation of pearson_r_log for each feature
    across specified p values and aggregated data, then prints results ranked by aggregated average.

    Parameters:
    corr_feat_df: DataFrame with columns 'feature', 'pearson_r_log', and 'p'
    p_values: list of p values to analyze (default [1, 3, 5, 10])

    Returns:
    combined_df: DataFrame with organized results
    """

    # Check that required columns exist
    required_cols = {'feature', 'pearson_r_log', 'p'}
    if not required_cols.issubset(corr_feat_df.columns):
        missing = required_cols - set(corr_feat_df.columns)
        raise ValueError(f"DataFrame must contain columns: {required_cols}. Missing: {missing}")

    # Get unique features
    features = sorted(corr_feat_df['feature'].unique())

    # Filter to only include rows with p in the specified p_values
    filtered_df = corr_feat_df[corr_feat_df['p'].isin(p_values)].copy()

    # Initialize list to store results
    results = []

    # Calculate mean and std per feature for each individual p value
    for p in p_values:
        df_p = filtered_df[filtered_df['p'] == p]
        for feature in features:
            feature_data = df_p[df_p['feature'] == feature]['pearson_r_log'].dropna()

            if not feature_data.empty:
                mean_val = feature_data.mean()
                std_val = feature_data.std()
                count_val = len(feature_data)
            else:
                mean_val, std_val, count_val = np.nan, np.nan, 0

            results.append({
                'feature': feature,
                'p': f'p={p}',
                'avg_pearson_r_log': mean_val,
                'std_pearson_r_log': std_val,
                'count': count_val
            })

    # Calculate aggregated stats for each feature across all selected p values
    for feature in features:
        feature_data = filtered_df[filtered_df['feature'] == feature]['pearson_r_log'].dropna()

        if not feature_data.empty:
            mean_val = feature_data.mean()
            std_val = feature_data.std()
            count_val = len(feature_data)
        else:
            mean_val, std_val, count_val = np.nan, np.nan, 0

        results.append({
            'feature': feature,
            'p': 'Aggregated',
            'avg_pearson_r_log': mean_val,
            'std_pearson_r_log': std_val,
            'count': count_val
        })

    # Convert results to DataFrame
    results_df = pd.DataFrame(results)

    # Create pivot tables for average and standard deviation
    avg_pivot = results_df.pivot(index='feature', columns='p', values='avg_pearson_r_log')
    std_pivot = results_df.pivot(index='feature', columns='p', values='std_pearson_r_log')
    count_pivot = results_df.pivot(index='feature', columns='p', values='count')

    # Combine average and std for display
    combined_df = pd.DataFrame()

    # Order columns: individual p values first, then aggregated
    column_order = [f'p={p}' for p in sorted(p_values)] + ['Aggregated']

    for col in column_order:
        if col in avg_pivot.columns:
            combined_df[f'{col} avg'] = avg_pivot[col]
            combined_df[f'{col} std'] = std_pivot[col]

    # Sort by aggregated average (lowest to highest)
    combined_df = combined_df.sort_values(by='Aggregated avg')

    # Print results in a neat format
    print("=" * 120)
    print("PEARSON R LOG STATISTICS BY FEATURE AND P VALUES")
    print("=" * 120)
    print(f"Analysis includes p values: {p_values}")
    print(f"Total features analyzed: {len(features)}")
    print(f"Total data points used: {len(filtered_df)}")
    print("\nResults sorted by Aggregated average (lowest to highest):")
    print("-" * 120)

    # Print the combined DataFrame with formatting
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    pd.set_option('display.max_rows', None)
    print(combined_df.to_string(float_format='{:8.4f}'.format))

    print("-" * 120)

    # Print summary statistics
    if 'Aggregated avg' in combined_df.columns:
        agg_avg = combined_df['Aggregated avg'].dropna()
        if not agg_avg.empty:
            print(f"\nSUMMARY STATISTICS FOR AGGREGATED AVERAGES:")
            print(f"Lowest average:   {agg_avg.min():8.4f} (Feature: {agg_avg.idxmin()})")
            print(f"Highest average:  {agg_avg.max():8.4f} (Feature: {agg_avg.idxmax()})")
            print(f"Mean of averages: {agg_avg.mean():8.4f}")
            print(f"Std of averages:  {agg_avg.std():8.4f}")

            # Show top 5 lowest and highest
            print(f"\nTOP 5 FEATURES WITH LOWEST AGGREGATED AVERAGES:")
            for i, (feature, avg) in enumerate(agg_avg.head(5).items(), 1):
                print(f"{i:2d}. {feature:30s}: {avg:8.4f}")

            print(f"\nTOP 5 FEATURES WITH HIGHEST AGGREGATED AVERAGES:")
            for i, (feature, avg) in enumerate(agg_avg.tail(5).items(), 1):
                print(f"{i:2d}. {feature:30s}: {avg:8.4f}")

    print("=" * 120)

    return combined_df

# Usage:
if 'corr_feat_df' not in locals() or corr_feat_df.empty:
    print("corr_feat_df DataFrame is not available or is empty. Please ensure it is loaded.")
else:
    # Check the structure of the dataframe
    print("DataFrame columns:", corr_feat_df.columns.tolist())
    print("DataFrame shape:", corr_feat_df.shape)

    if 'feature' in corr_feat_df.columns:
        print(f"Number of unique features: {corr_feat_df['feature'].nunique()}")
        print("Sample features:", corr_feat_df['feature'].unique()[:10].tolist())

    if 'p' in corr_feat_df.columns:
        print(f"Available p values: {sorted(corr_feat_df['p'].unique())}")

    print("\nComputing statistics...")

    # Compute and display the statistics
    try:
        combined_statistics = compute_and_rank_pearson_stats(corr_feat_df)
    except ValueError as e:
        print(f"Error: {e}")
        print("Please check that your DataFrame has the required columns: 'feature', 'pearson_r_log', 'p'")

import pandas as pd
import numpy as np

def compute_and_rank_pearson_stats_with_cluster_std(corr_feat_df_vals, p_values=[1, 3, 5, 10], cluster_col=None):
    """
    Computes comprehensive statistics for pearson_r_log by feature and p values including:
    1. Average and std of pearson_r_log per feature for each p value
    2. Aggregated stats across selected p values
    3. Mean of cluster standard deviations (if cluster identifier exists)

    Parameters:
    corr_feat_df_vals: DataFrame with columns 'feature', 'pearson_r_log', 'p', and optionally cluster identifier
    p_values: list of p values to analyze (default [1, 3, 5, 10])
    cluster_col: name of column identifying clusters (if None, assumes each row is a separate cluster)

    Returns:
    combined_df: DataFrame with organized results
    """

    # Check that required columns exist
    required_cols = {'feature', 'pearson_r_log', 'p'}
    if not required_cols.issubset(corr_feat_df_vals.columns):
        missing = required_cols - set(corr_feat_df_vals.columns)
        raise ValueError(f"DataFrame must contain columns: {required_cols}. Missing: {missing}")

    # Get unique features
    features = sorted(corr_feat_df_vals['feature'].unique())

    # Filter to only include rows with p in the specified p_values
    filtered_df = corr_feat_df_vals[corr_feat_df_vals['p'].isin(p_values)].copy()

    # Initialize list to store results
    results = []

    # Calculate stats per feature for each individual p value
    for p in p_values:
        df_p = filtered_df[filtered_df['p'] == p]
        for feature in features:
            feature_data = df_p[df_p['feature'] == feature]['pearson_r_log'].dropna()

            if not feature_data.empty:
                mean_val = feature_data.mean()
                std_val = feature_data.std()
                count_val = len(feature_data)

                # Compute mean cluster std deviation
                if cluster_col and cluster_col in df_p.columns:
                    # Group by cluster and compute std for each cluster, then take mean
                    cluster_stds = []
                    feature_clusters = df_p[df_p['feature'] == feature]
                    for cluster_id in feature_clusters[cluster_col].unique():
                        cluster_data = feature_clusters[feature_clusters[cluster_col] == cluster_id]['pearson_r_log'].dropna()
                        if len(cluster_data) > 1:
                            cluster_stds.append(cluster_data.std())

                    mean_cluster_std = np.mean(cluster_stds) if cluster_stds else np.nan
                else:
                    # If no cluster column, treat each observation as from a different cluster (std = 0)
                    # or use the overall std as approximation
                    mean_cluster_std = std_val  # This is the std across all "clusters" (observations)
            else:
                mean_val, std_val, count_val, mean_cluster_std = np.nan, np.nan, 0, np.nan

            results.append({
                'feature': feature,
                'p': f'p={p}',
                'avg_pearson_r_log': mean_val,
                'std_pearson_r_log': std_val,
                'mean_cluster_std': mean_cluster_std,
                'count': count_val
            })

    # Calculate aggregated stats for each feature across all selected p values
    for feature in features:
        feature_data = filtered_df[filtered_df['feature'] == feature]['pearson_r_log'].dropna()

        if not feature_data.empty:
            mean_val = feature_data.mean()
            std_val = feature_data.std()
            count_val = len(feature_data)

            # Compute mean cluster std deviation for aggregated data
            if cluster_col and cluster_col in filtered_df.columns:
                cluster_stds = []
                feature_clusters = filtered_df[filtered_df['feature'] == feature]
                for cluster_id in feature_clusters[cluster_col].unique():
                    cluster_data = feature_clusters[feature_clusters[cluster_col] == cluster_id]['pearson_r_log'].dropna()
                    if len(cluster_data) > 1:
                        cluster_stds.append(cluster_data.std())

                mean_cluster_std = np.mean(cluster_stds) if cluster_stds else np.nan
            else:
                mean_cluster_std = std_val
        else:
            mean_val, std_val, count_val, mean_cluster_std = np.nan, np.nan, 0, np.nan

        results.append({
            'feature': feature,
            'p': 'Aggregated',
            'avg_pearson_r_log': mean_val,
            'std_pearson_r_log': std_val,
            'mean_cluster_std': mean_cluster_std,
            'count': count_val
        })

    # Convert results to DataFrame
    results_df = pd.DataFrame(results)

    # Create pivot tables for each metric
    avg_pivot = results_df.pivot(index='feature', columns='p', values='avg_pearson_r_log')
    std_pivot = results_df.pivot(index='feature', columns='p', values='std_pearson_r_log')
    cluster_std_pivot = results_df.pivot(index='feature', columns='p', values='mean_cluster_std')
    count_pivot = results_df.pivot(index='feature', columns='p', values='count')

    # Combine all metrics for display
    combined_df = pd.DataFrame()

    # Order columns: individual p values first, then aggregated
    column_order = [f'p={p}' for p in sorted(p_values)] + ['Aggregated']

    for col in column_order:
        if col in avg_pivot.columns:
            combined_df[f'{col} avg'] = avg_pivot[col]
            combined_df[f'{col} std'] = std_pivot[col]
            combined_df[f'{col} cluster_std'] = cluster_std_pivot[col]

    # Sort by aggregated average (lowest to highest)
    combined_df = combined_df.sort_values(by='Aggregated avg')

    # Print results in a neat format
    print("=" * 140)
    print("COMPREHENSIVE PEARSON R LOG STATISTICS BY FEATURE AND P VALUES")
    print("=" * 140)
    print(f"Analysis includes p values: {p_values}")
    print(f"Total features analyzed: {len(features)}")
    print(f"Total data points used: {len(filtered_df)}")
    if cluster_col:
        print(f"Cluster column used: {cluster_col}")
    else:
        print("No cluster column specified - treating each observation as separate cluster")
    print("\nColumns explained:")
    print("  - 'avg': Mean of pearson_r_log values for the feature")
    print("  - 'std': Standard deviation of pearson_r_log values for the feature")
    print("  - 'cluster_std': Mean of standard deviations within clusters for the feature")
    print("\nResults sorted by Aggregated average (lowest to highest):")
    print("-" * 140)

    # Print the combined DataFrame with formatting
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    pd.set_option('display.max_rows', None)
    print(combined_df.to_string(float_format='{:8.4f}'.format))

    print("-" * 140)

    # Print summary statistics
    if 'Aggregated avg' in combined_df.columns:
        agg_avg = combined_df['Aggregated avg'].dropna()
        agg_cluster_std = combined_df['Aggregated cluster_std'].dropna()

        if not agg_avg.empty:
            print(f"\nSUMMARY STATISTICS:")
            print(f"\nAggregated Averages:")
            print(f"  Lowest average:   {agg_avg.min():8.4f} (Feature: {agg_avg.idxmin()})")
            print(f"  Highest average:  {agg_avg.max():8.4f} (Feature: {agg_avg.idxmax()})")
            print(f"  Mean of averages: {agg_avg.mean():8.4f}")
            print(f"  Std of averages:  {agg_avg.std():8.4f}")

            if not agg_cluster_std.empty:
                print(f"\nAggregated Cluster Standard Deviations:")
                print(f"  Lowest cluster std:  {agg_cluster_std.min():8.4f} (Feature: {agg_cluster_std.idxmin()})")
                print(f"  Highest cluster std: {agg_cluster_std.max():8.4f} (Feature: {agg_cluster_std.idxmax()})")
                print(f"  Mean cluster std:    {agg_cluster_std.mean():8.4f}")

            # Show top 5 lowest and highest averages
            print(f"\nTOP 5 FEATURES WITH LOWEST AGGREGATED AVERAGES:")
            for i, (feature, avg) in enumerate(agg_avg.head(5).items(), 1):
                cluster_std_val = combined_df.loc[feature, 'Aggregated cluster_std']
                print(f"{i:2d}. {feature:40s}: avg={avg:8.4f}, cluster_std={cluster_std_val:8.4f}")

            print(f"\nTOP 5 FEATURES WITH HIGHEST AGGREGATED AVERAGES:")
            for i, (feature, avg) in enumerate(agg_avg.tail(5).items(), 1):
                cluster_std_val = combined_df.loc[feature, 'Aggregated cluster_std']
                print(f"{i:2d}. {feature:40s}: avg={avg:8.4f}, cluster_std={cluster_std_val:8.4f}")

    print("=" * 140)

    return combined_df

# Usage:
if 'corr_feat_df_vals' not in locals() or corr_feat_df_vals.empty:
    print("corr_feat_df_vals DataFrame is not available or is empty. Please ensure it is loaded.")
else:
    # Check the structure of the dataframe
    print("DataFrame columns:", corr_feat_df_vals.columns.tolist())
    print("DataFrame shape:", corr_feat_df_vals.shape)

    if 'feature' in corr_feat_df_vals.columns:
        print(f"Number of unique features: {corr_feat_df_vals['feature'].nunique()}")
        print("Sample features:", corr_feat_df_vals['feature'].unique()[:10].tolist())

    if 'p' in corr_feat_df_vals.columns:
        print(f"Available p values: {sorted(corr_feat_df_vals['p'].unique())}")

    # Check for potential cluster columns
    potential_cluster_cols = [col for col in corr_feat_df_vals.columns
                             if 'cluster' in col.lower() or 'id' in col.lower()]
    if potential_cluster_cols:
        print(f"Potential cluster columns found: {potential_cluster_cols}")
        cluster_col = potential_cluster_cols[0]  # Use the first one found
    else:
        cluster_col = None
        print("No cluster column detected - will treat each row as separate cluster")

    print("\nComputing comprehensive statistics...")

    # Compute and display the statistics
    try:
        combined_statistics = compute_and_rank_pearson_stats_with_cluster_std(
            corr_feat_df_vals,
            p_values=[1, 3, 5, 10],
            cluster_col=cluster_col
        )
    except ValueError as e:
        print(f"Error: {e}")
        print("Please check that your DataFrame has the required columns: 'feature', 'pearson_r_log', 'p'")

